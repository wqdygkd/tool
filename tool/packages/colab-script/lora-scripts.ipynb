{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "## 克隆仓库并安装依赖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbbbbbbbb",
        "outputId": "435ba0d3-730c-4a9e-8266-3ac172485460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "-= Initial setup lora-scripts =-\n",
            "Cloning into 'lora-scripts'...\n",
            "remote: Enumerating objects: 2429, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 2429 (delta 107), reused 86 (delta 86), pack-reused 2304 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2429/2429), 1.87 MiB | 9.41 MiB/s, done.\n",
            "Resolving deltas: 100% (1509/1509), done.\n",
            "Updating files: 100% (275/275), done.\n",
            "Submodule 'frontend' (https://github.com/hanamizuki-ai/lora-gui-dist) registered for path 'frontend'\n",
            "Submodule 'mikazuki/dataset-tag-editor' (https://github.com/Akegarasu/dataset-tag-editor) registered for path 'mikazuki/dataset-tag-editor'\n",
            "Cloning into '/content/drive/MyDrive/lora-scripts/frontend'...\n",
            "remote: Enumerating objects: 2405, done.        \n",
            "remote: Counting objects: 100% (1529/1529), done.        \n",
            "remote: Compressing objects: 100% (829/829), done.        \n",
            "remote: Total 2405 (delta 884), reused 1259 (delta 624), pack-reused 876 (from 1)        \n",
            "Receiving objects: 100% (2405/2405), 10.88 MiB | 9.24 MiB/s, done.\n",
            "Resolving deltas: 100% (1553/1553), done.\n",
            "Cloning into '/content/drive/MyDrive/lora-scripts/mikazuki/dataset-tag-editor'...\n",
            "remote: Enumerating objects: 326, done.        \n",
            "remote: Counting objects: 100% (91/91), done.        \n",
            "remote: Compressing objects: 100% (16/16), done.        \n",
            "remote: Total 326 (delta 75), reused 75 (delta 75), pack-reused 235 (from 1)        \n",
            "Receiving objects: 100% (326/326), 1.33 MiB | 13.09 MiB/s, done.\n",
            "Resolving deltas: 100% (184/184), done.\n",
            "Submodule path 'frontend': checked out 'a61f4b8b7409e4bf5a94bbdad18de1a89519487f'\n",
            "Submodule path 'mikazuki/dataset-tag-editor': checked out '8ee3a6f30c046f3c89c531dc1abbff2d2a466bcf'\n",
            "/content/drive/MyDrive/lora-scripts\n",
            "-= Install dependencies =-\n",
            "Creating python venv...\n",
            "Error: Command '['/content/drive/MyDrive/lora-scripts/venv/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n",
            "./install.bash: line 21: /content/drive/MyDrive/lora-scripts/venv/bin/activate: No such file or directory\n",
            "active venv\n",
            "Installing torch & xformers...\n",
            "./install.bash: line 27: nvidia-smi: command not found\n",
            "CUDA Version: 12.5\n",
            "install torch 2.4.1+cu124\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
            "Collecting torch==2.4.1+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.4.1%2Bcu124-cp311-cp311-linux_x86_64.whl (797.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.1+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.19.1%2Bcu124-cp311-cp311-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1+cu124) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1+cu124) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1+cu124) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1+cu124) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (24.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.4/883.4 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.99 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# #@title Environment Setup\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "\n",
        "current_dir = !pwd\n",
        "WORKSPACE = f\"{current_dir[0]}/lora-scripts\"\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/lora-scripts\"\n",
        "    %cd /content/drive/MyDrive\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup lora-scripts =- && git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts.git\n",
        "%cd $WORKSPACE\n",
        "\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!bash ./install.bash\n",
        "\n",
        "# !pip3 install accelerate\n",
        "# !pip3 install einops transformers>=4.28.1 safetensors>=0.4.2 aiohttp pyyaml Pillow scipy tqdm psutil tokenizers>=0.13.3\n",
        "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip3 install torchsde\n",
        "# !pip3 install kornia>=0.7.1 spandrel soundfile sentencepiece\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "## 下载模型"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://huggingface.co/Kijai/flux-fp8/resolve/main/flux1-dev-fp8.safetensors -P /content/models/unet/"
      ],
      "metadata": {
        "id": "l3JNulC9YtrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors -P /content/models/text_encoders/"
      ],
      "metadata": {
        "id": "59bM-KZYYx2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors -P /content/models/text_encoders/"
      ],
      "metadata": {
        "id": "J9Z8p5iAY2iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/ae.safetensors -P /content/models/vae/"
      ],
      "metadata": {
        "id": "FSaewT4mY7UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with cloudflared (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjjj"
      },
      "outputs": [],
      "source": [
        "!wget -P ~ https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i ~/cloudflared-linux-amd64.deb\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
        "\n",
        "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  for line in p.stderr:\n",
        "    l = line.decode()\n",
        "    if \"trycloudflare.com \" in l:\n",
        "      print(\"This is the URL to access ComfyUI:\", l[l.find(\"http\"):], end='')\n",
        "    #print(l, end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with localtunnel\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjj"
      },
      "outputs": [],
      "source": [
        "!npm install -g localtunnel\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
        "\n",
        "  print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
        "  for line in p.stdout:\n",
        "    print(line.decode(), end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(28000,)).start()\n",
        "\n",
        "!bash run_gui.sh --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gggggggggg"
      },
      "source": [
        "### Run ComfyUI with colab iframe (use only in case the previous way with localtunnel doesn't work)\n",
        "\n",
        "You should see the ui appear in an iframe. If you get a 403 error, it's your firefox settings or an extension that's messing things up.\n",
        "\n",
        "If you want to open it in another window use the link.\n",
        "\n",
        "Note that some UI features like live image previews won't work because the colab iframe blocks websockets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhhhhhhhh"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "import socket\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  from google.colab import output\n",
        "  output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "  print(\"to open it in a window you can open this link here:\")\n",
        "  output.serve_kernel_port_as_window(port)\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}