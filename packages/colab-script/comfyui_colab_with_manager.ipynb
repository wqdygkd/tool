{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wqdygkd/tool/blob/main/packages/colab-script/comfyui_colab_with_manager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "初始化存储\n",
        "Git clone the repo and install the requirements. (ignore the pip errors about protobuf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbbbbbbbb",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b5a060-c931-418b-a7c2-449192badaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/ComfyUI\n",
            "-= Updating ComfyUI =-\n",
            "Already up to date.\n",
            "-= Install dependencies =-\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchsde in /usr/local/lib/python3.11/dist-packages (0.2.6)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from torchsde) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.11/dist-packages (from torchsde) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from torchsde) (2.6.0+cu124)\n",
            "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.11/dist-packages (from torchsde) (0.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->torchsde) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->torchsde) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->torchsde) (3.0.2)\n",
            "/content/drive/MyDrive/ComfyUI/custom_nodes\n",
            "/content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager\n",
            "Updating 908a1009..50fc1389\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tcustom-node-list.json\n",
            "Please commit your changes or stash them before you merge.\n",
            "Aborting\n",
            "/content/drive/MyDrive/ComfyUI\n",
            "-= Install custom nodes dependencies =-\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (3.1.44)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.2)\n",
            "\n",
            "\u001b[1;33mWARN: The `COMFYUI_PATH` environment variable is not set. Assuming \u001b[0m\n",
            "\u001b[1;33m`custom_nodes/ComfyUI-Manager/..\u001b[0m\u001b[1;33m/../\u001b[0m\u001b[1;33m` as the ComfyUI path.\u001b[0m\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Restoring \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m/\u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m: \u001b[35m/content/drive/MyDrive/ComfyUI/custom_nodes/\u001b[0m\u001b[95mComfyUI-Manager\u001b[0m\n",
            "Install: pip packages\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'GitPython'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (3.1.44)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.2)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'PyGithub'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (4.13.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.26.20)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.2.18)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (2025.1.31)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'matrix-\u001b[0m\u001b[32mclient\u001b[0m\u001b[32m==0.4.0'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: matrix-client==0.4.0 in /usr/local/lib/python3.11/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests~=2.22 in /usr/local/lib/python3.11/dist-packages (from matrix-client==0.4.0) (2.32.3)\n",
            "Requirement already satisfied: urllib3~=1.21 in /usr/local/lib/python3.11/dist-packages (from matrix-client==0.4.0) (1.26.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.22->matrix-client==0.4.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.22->matrix-client==0.4.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.22->matrix-client==0.4.0) (2025.1.31)\n",
            "\u001b[1m[\u001b[0mComfyUI-Manager\u001b[1m]\u001b[0m skip black listed pip installation: \u001b[32m'transformers'\u001b[0m\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'huggingface-hub>0.20'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: huggingface-hub>0.20 in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>0.20) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>0.20) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>0.20) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>0.20) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>0.20) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>0.20) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>0.20) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>0.20) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>0.20) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>0.20) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>0.20) (2025.1.31)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'typer'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer) (8.1.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer) (4.13.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer) (0.1.2)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'rich'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'typing-extensions'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.13.0)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'toml'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'uv'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: uv in /usr/local/lib/python3.11/dist-packages (0.6.12)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'chardet'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Restoring \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m/\u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m: \u001b[35m/content/drive/MyDrive/ComfyUI/custom_nodes/\u001b[0m\u001b[95m__pycache__\u001b[0m\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Restoring \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m/\u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m: \u001b[35m/content/drive/MyDrive/ComfyUI/custom_nodes/\u001b[0m\u001b[95mrgthree-comfy\u001b[0m\n",
            "Install: pip packages\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Restoring \u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m/\u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m: \u001b[35m/content/drive/MyDrive/ComfyUI/custom_nodes/\u001b[0m\u001b[95mwas-node-suite-comfyui\u001b[0m\n",
            "Install: pip packages\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'cmake'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (3.31.6)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'fairscale>=0.4.4'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Collecting fairscale>=0.4.4\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from fairscale>=0.4.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from fairscale>=0.4.4) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale>=0.4.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->fairscale>=0.4.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->fairscale>=0.4.4) (3.0.2)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332207 sha256=cf9cdd43d3a423e645b084f469178a763eb1bf221ad5d9ca16d91f16cc5549e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/ef/96/5044bde220b2ea299bdc6ec05051e0ef187fad45b341d1c273\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.13\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \n",
            "\u001b[32m'git+https://github.com/WASasquatch/img2texture.git'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Collecting git+https://github.com/WASasquatch/img2texture.git\n",
            "  Cloning https://github.com/WASasquatch/img2texture.git to /tmp/pip-req-build-t3ru25pn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/WASasquatch/img2texture.git /tmp/pip-req-build-t3ru25pn\n",
            "  Resolved https://github.com/WASasquatch/img2texture.git to commit d6159abea44a0b2cf77454d3d46962c8b21eb9d3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from img2texture==1.0.6) (11.1.0)\n",
            "Building wheels for collected packages: img2texture\n",
            "  Building wheel for img2texture (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for img2texture: filename=img2texture-1.0.6-py3-none-any.whl size=8357 sha256=9fe1092b171626d9783bf2deb1a9e4210b422312217ea6cf2b509b5f350d686c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rxvu8mzo/wheels/3a/3b/e7/ebce81e2912d6a6e38372603eda3b92fa14d0c95eb2bb56e45\n",
            "Successfully built img2texture\n",
            "Installing collected packages: img2texture\n",
            "Successfully installed img2texture-1.0.6\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \n",
            "\u001b[32m'git+https://github.com/WASasquatch/cstr'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Collecting git+https://github.com/WASasquatch/cstr\n",
            "  Cloning https://github.com/WASasquatch/cstr to /tmp/pip-req-build-eh1_4_99\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/WASasquatch/cstr /tmp/pip-req-build-eh1_4_99\n",
            "  Resolved https://github.com/WASasquatch/cstr to commit 0520c29a18a7a869a6e5983861d6f7a4c86f8e9b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: cstr\n",
            "  Building wheel for cstr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cstr: filename=cstr-0.1.0-py3-none-any.whl size=2828 sha256=9c67155e34eb022a4c7a7f0a05e67a639ad2da64aa4da8f81430b3a6934bfad3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-66ipmmcb/wheels/99/a1/f2/03f7c4ed7d0280528ae4a07e4dacd6d32a8ff90c1783a4c2e5\n",
            "Successfully built cstr\n",
            "Installing collected packages: cstr\n",
            "Successfully installed cstr-0.1.0\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'gitpython'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (3.1.44)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.2)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'imageio'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.1.0)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'joblib'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'matplotlib'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'numba'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (1.26.4)\n",
            "\u001b[1m[\u001b[0mComfyUI-Manager\u001b[1m]\u001b[0m \u001b[32m'numpy'\u001b[0m is remapped to \u001b[32m'numpy<2'\u001b[0m\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'numpy<2'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \n",
            "\u001b[32m'opencv-python-headless\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: opencv-python-headless[ffmpeg] in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "\u001b[33mWARNING: opencv-python-headless 4.11.0.86 does not provide the extra 'ffmpeg'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless[ffmpeg]) (1.26.4)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'pilgram'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Collecting pilgram\n",
            "  Downloading pilgram-1.2.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pilgram) (1.26.4)\n",
            "Downloading pilgram-1.2.1-py3-none-any.whl (817 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.4/817.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pilgram\n",
            "Successfully installed pilgram-1.2.1\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \n",
            "\u001b[32m'git+https://github.com/WASasquatch/ffmpy.git'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Collecting git+https://github.com/WASasquatch/ffmpy.git\n",
            "  Cloning https://github.com/WASasquatch/ffmpy.git to /tmp/pip-req-build-dl71cg1y\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/WASasquatch/ffmpy.git /tmp/pip-req-build-dl71cg1y\n",
            "  Resolved https://github.com/WASasquatch/ffmpy.git to commit f000737698b387ffaeab7cd871b0e9185811230d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=5496 sha256=389d02b3ae5afedd7b5480b1bdf528efb5f79602826c5916965508a0d9b7bf09\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qhiltah4/wheels/30/98/4f/464a8020203e4f2c1049cf1d48bd93f687e76bc34b6b378923\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: ffmpy\n",
            "Successfully installed ffmpy-0.3.0\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'rembg'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Collecting rembg\n",
            "  Downloading rembg-2.0.65-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from rembg) (4.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rembg) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from rembg) (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from rembg) (11.1.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from rembg) (1.8.2)\n",
            "Collecting pymatting (from rembg)\n",
            "  Downloading PyMatting-1.1.13-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from rembg) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from rembg) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from rembg) (4.67.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->rembg) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->rembg) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->rembg) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->rembg) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch->rembg) (4.3.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch->rembg) (24.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch->rembg) (2.32.3)\n",
            "Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.11/dist-packages (from pymatting->rembg) (0.60.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->rembg) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->rembg) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->rembg) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->rembg) (0.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.43.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema->rembg) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->rembg) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->rembg) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->rembg) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->rembg) (2025.1.31)\n",
            "Downloading rembg-2.0.65-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMatting-1.1.13-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymatting, rembg\n",
            "Successfully installed pymatting-1.1.13 rembg-2.0.65\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'scikit-image>=0.20.0'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: scikit-image>=0.20.0 in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.20.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.20.0) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.20.0) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.20.0) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.20.0) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.20.0) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.20.0) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.20.0) (0.4)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'scikit-learn'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'scipy'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'timm>=0.4.12'\u001b[0m\u001b[1m]\u001b[0m\n",
            "Requirement already satisfied: timm>=0.4.12 in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm>=0.4.12) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm>=0.4.12) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm>=0.4.12) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm>=0.4.12) (0.30.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.4.12) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.4.12) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.4.12) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.4.12) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.4.12) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.4.12) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.4.12) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=0.4.12) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm>=0.4.12) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm>=0.4.12) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm>=0.4.12) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm>=0.4.12) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm>=0.4.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm>=0.4.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm>=0.4.12) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm>=0.4.12) (2025.1.31)\n",
            "\n",
            "## ComfyUI-Manager: EXECUTE => \u001b[1m[\u001b[0m\u001b[32m'/usr/bin/python3'\u001b[0m, \u001b[32m'-m'\u001b[0m, \u001b[32m'pip'\u001b[0m, \u001b[32m'install'\u001b[0m, \u001b[32m'tqdm'\u001b[0m\u001b[1m]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# #@title Environment Setup\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\n",
        "USE_COMFYUI_MANAGER = True  #@param {type:\"boolean\"}\n",
        "INSTALL_CUSTOM_NODES_DEPENDENCIES = True  #@param {type:\"boolean\"}\n",
        "USE_ONEDRIVE = False # @param {\"type\":\"boolean\"}\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "OPTIONS['USE_ONEDRIVE'] = USE_ONEDRIVE\n",
        "OPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n",
        "OPTIONS['USE_COMFYUI_MANAGER'] = USE_COMFYUI_MANAGER\n",
        "OPTIONS['INSTALL_CUSTOM_NODES_DEPENDENCIES'] = INSTALL_CUSTOM_NODES_DEPENDENCIES\n",
        "\n",
        "current_dir = !pwd\n",
        "WORKSPACE = f\"{current_dir[0]}/ComfyUI\"\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive\n",
        "\n",
        "if OPTIONS['USE_ONEDRIVE']:\n",
        "    !echo \"初始化onedrive...\"\n",
        "    %cd /\n",
        "\n",
        "    !curl  https://rclone.org/install.sh | sudo bash\n",
        "    !rm rclone.conf\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    !mkdir /root/.config/rclone/\n",
        "    !cp rclone.conf /root/.config/rclone/rclone.conf\n",
        "    !sudo apt install fuse3 -y\n",
        "\n",
        "    !sudo mkdir -p /content/onedrive2\n",
        "    !nohup rclone --vfs-cache-mode writes mount onedrive: /content/onedrive2  &\n",
        "    !sleep 2\n",
        "    !ls /content/onedrive2/AI -la\n",
        "\n",
        "    WORKSPACE = \"/content/onedrive2/AI/ComfyUI\"\n",
        "    %cd /content/onedrive2/AI\n",
        "\n",
        "\n",
        "    #!fusermount -qzu /content/drive\n",
        "    #!sudo umount -l /content/drive\n",
        "    #!rm -r /content/drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['UPDATE_COMFY_UI']:\n",
        "  !echo -= Updating ComfyUI =-\n",
        "\n",
        "  # Correction of the issue of permissions being deleted on Google Drive.\n",
        "  ![ -f \".ci/nightly/update_windows/update_comfyui_and_python_dependencies.bat\" ] && chmod 755 .ci/nightly/update_windows/update_comfyui_and_python_dependencies.bat\n",
        "  ![ -f \".ci/nightly/windows_base_files/run_nvidia_gpu.bat\" ] && chmod 755 .ci/nightly/windows_base_files/run_nvidia_gpu.bat\n",
        "  ![ -f \".ci/update_windows/update_comfyui_and_python_dependencies.bat\" ] && chmod 755 .ci/update_windows/update_comfyui_and_python_dependencies.bat\n",
        "  ![ -f \".ci/update_windows_cu118/update_comfyui_and_python_dependencies.bat\" ] && chmod 755 .ci/update_windows_cu118/update_comfyui_and_python_dependencies.bat\n",
        "  ![ -f \".ci/update_windows/update.py\" ] && chmod 755 .ci/update_windows/update.py\n",
        "  ![ -f \".ci/update_windows/update_comfyui.bat\" ] && chmod 755 .ci/update_windows/update_comfyui.bat\n",
        "  ![ -f \".ci/update_windows/README_VERY_IMPORTANT.txt\" ] && chmod 755 .ci/update_windows/README_VERY_IMPORTANT.txt\n",
        "  ![ -f \".ci/update_windows/run_cpu.bat\" ] && chmod 755 .ci/update_windows/run_cpu.bat\n",
        "  ![ -f \".ci/update_windows/run_nvidia_gpu.bat\" ] && chmod 755 .ci/update_windows/run_nvidia_gpu.bat\n",
        "\n",
        "  !git pull\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!pip3 install accelerate\n",
        "!pip3 install einops transformers>=4.28.1 safetensors>=0.4.2 aiohttp pyyaml Pillow scipy tqdm psutil tokenizers>=0.13.3\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip3 install torchsde\n",
        "!pip3 install kornia>=0.7.1 spandrel soundfile sentencepiece\n",
        "\n",
        "if OPTIONS['USE_COMFYUI_MANAGER']:\n",
        "  %cd custom_nodes\n",
        "\n",
        "  # Correction of the issue of permissions being deleted on Google Drive.\n",
        "  ![ -f \"ComfyUI-Manager/check.sh\" ] && chmod 755 ComfyUI-Manager/check.sh\n",
        "  ![ -f \"ComfyUI-Manager/scan.sh\" ] && chmod 755 ComfyUI-Manager/scan.sh\n",
        "  ![ -f \"ComfyUI-Manager/node_db/dev/scan.sh\" ] && chmod 755 ComfyUI-Manager/node_db/dev/scan.sh\n",
        "  ![ -f \"ComfyUI-Manager/node_db/tutorial/scan.sh\" ] && chmod 755 ComfyUI-Manager/node_db/tutorial/scan.sh\n",
        "  ![ -f \"ComfyUI-Manager/scripts/install-comfyui-venv-linux.sh\" ] && chmod 755 ComfyUI-Manager/scripts/install-comfyui-venv-linux.sh\n",
        "  ![ -f \"ComfyUI-Manager/scripts/install-comfyui-venv-win.bat\" ] && chmod 755 ComfyUI-Manager/scripts/install-comfyui-venv-win.bat\n",
        "\n",
        "  ![ ! -d ComfyUI-Manager ] && echo -= Initial setup ComfyUI-Manager =- && git clone https://github.com/ltdrdata/ComfyUI-Manager\n",
        "  %cd ComfyUI-Manager\n",
        "  !git pull\n",
        "\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['INSTALL_CUSTOM_NODES_DEPENDENCIES']:\n",
        "  !echo -= Install custom nodes dependencies =-\n",
        "  !pip install GitPython\n",
        "  !python custom_nodes/ComfyUI-Manager/cm-cli.py restore-dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download some models/checkpoints/vae or custom comfyui nodes (uncomment the commands for the ones you want)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://civitai.com/api/download/models/618692 -O ./models/uned/flux_dev.safetensors"
      ],
      "metadata": {
        "id": "TH6pRbSy7AtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://huggingface.co/Kijai/flux-fp8/resolve/main/flux1-dev-fp8.safetensors -P ./models/unet/"
      ],
      "metadata": {
        "id": "MRZkDVkO7IvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoints\n",
        "\n",
        "# flux1-dev-fp8\n",
        "#!wget -c https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# flux1-dev-fp8\n",
        "!wget -c https://huggingface.co/Kijai/flux-fp8/resolve/main/flux1-dev-fp8.safetensors -P ./models/unet/\n",
        "#!wget -c https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# clip\n",
        "!wget -c https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors -P ./models/text_encoders/\n",
        "!wget -c https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors -P ./models/text_encoders/\n",
        "\n",
        "# VAE\n",
        "!wget -c https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/ae.safetensors -P ./models/vae/\n"
      ],
      "metadata": {
        "id": "um-bGwSvxJqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dddddddddd"
      },
      "outputs": [],
      "source": [
        "### SDXL\n",
        "### I recommend these workflow examples: https://comfyanonymous.github.io/ComfyUI_examples/sdxl/\n",
        "\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# SDXL ReVision\n",
        "#!wget -c https://huggingface.co/comfyanonymous/clip_vision_g/resolve/main/clip_vision_g.safetensors -P ./models/clip_vision/\n",
        "\n",
        "# SD1.5\n",
        "!wget -c https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt -P ./models/checkpoints/\n",
        "\n",
        "# SD2\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Some SD1.5 anime style\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
        "#!wget -c https://huggingface.co/waifu-diffusion/wd-1-5-beta3/resolve/main/wd-illusion-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# unCLIP models\n",
        "#!wget -c https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# VAE\n",
        "!wget -c https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/ae.safetensors -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -P ./models/vae/\n",
        "\n",
        "\n",
        "# Loras\n",
        "#!wget -c https://civitai.com/api/download/models/10350 -O ./models/loras/theovercomer8sContrastFix_sd21768.safetensors #theovercomer8sContrastFix SD2.x 768-v\n",
        "#!wget -c https://civitai.com/api/download/models/10638 -O ./models/loras/theovercomer8sContrastFix_sd15.safetensors #theovercomer8sContrastFix SD1.x\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors -P ./models/loras/ #SDXL offset noise lora\n",
        "\n",
        "\n",
        "# T2I-Adapter\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth -P ./models/controlnet/\n",
        "\n",
        "# T2I Styles Model\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth -P ./models/style_models/\n",
        "\n",
        "# CLIPVision model (needed for styles model)\n",
        "#!wget -c https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin -O ./models/clip_vision/clip_vit14.bin\n",
        "\n",
        "\n",
        "# ControlNet\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors -P ./models/controlnet/\n",
        "\n",
        "# ControlNet SDXL\n",
        "#!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-recolor-rank256.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-sketch-rank256.safetensors -P ./models/controlnet/\n",
        "\n",
        "# Controlnet Preprocessor nodes by Fannovel16\n",
        "#!cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n",
        "\n",
        "\n",
        "# GLIGEN\n",
        "#!wget -c https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors -P ./models/gligen/\n",
        "\n",
        "\n",
        "# ESRGAN upscale model\n",
        "#!wget -c https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -P ./models/upscale_models/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with cloudflared (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjjj"
      },
      "outputs": [],
      "source": [
        "!wget -P ~ https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i ~/cloudflared-linux-amd64.deb\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
        "\n",
        "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  for line in p.stderr:\n",
        "    l = line.decode()\n",
        "    if \"trycloudflare.com \" in l:\n",
        "      print(\"This is the URL to access ComfyUI:\", l[l.find(\"http\"):], end='')\n",
        "    #print(l, end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with localtunnel\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjj",
        "outputId": "24cdb4e5-10ca-4ce2-e922-4faab9633f1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight'])                                         \n",
            "import network module: .networks.lora_flux\n",
            "                    ERROR    !!! Exception during processing !!! Cannot copy out of execution.py:396\n",
            "                             meta tensor; no data! Please use                                       \n",
            "                             torch.nn.Module.to_empty() instead of                                  \n",
            "                             torch.nn.Module.to() when moving module from meta to a                 \n",
            "                             different device.                                                      \n",
            "                    ERROR    Traceback (most recent call last):                     execution.py:397\n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 327, in execute                                                   \n",
            "                                 output_data, output_ui, has_subgraph =                             \n",
            "                             get_output_data(obj, input_data_all,                                   \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                                        ^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                              \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 202, in get_output_data                                           \n",
            "                                 return_values = _map_node_over_list(obj,                           \n",
            "                             input_data_all, obj.FUNCTION, allow_interrupt=True,                    \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^                                                                   \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 174, in _map_node_over_list                                       \n",
            "                                 process_inputs(input_dict, i)                                      \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 163, in process_inputs                                            \n",
            "                                 results.append(getattr(obj, func)(**inputs))                       \n",
            "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                        \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/nodes.py\", line 631, in init_training                       \n",
            "                                 training_loop = network_trainer.init_train(args)                   \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                   \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/train_network.py\", line 449, in init_train                  \n",
            "                                 vae.to(accelerator.device, dtype=vae_dtype)                        \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 1343, in to                                       \n",
            "                                 return self._apply(convert)                                        \n",
            "                                        ^^^^^^^^^^^^^^^^^^^^                                        \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 903, in _apply                                    \n",
            "                                 module._apply(fn)                                                  \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 903, in _apply                                    \n",
            "                                 module._apply(fn)                                                  \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 930, in _apply                                    \n",
            "                                 param_applied = fn(param)                                          \n",
            "                                                 ^^^^^^^^^                                          \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 1336, in convert                                  \n",
            "                                 raise NotImplementedError(                                         \n",
            "                             NotImplementedError: Cannot copy out of meta tensor;                   \n",
            "                             no data! Please use torch.nn.Module.to_empty() instead                 \n",
            "                             of torch.nn.Module.to() when moving module from meta                   \n",
            "                             to a different device.                                                 \n",
            "                                                                                                    \n",
            "                    INFO     Prompt executed in 3.82 seconds                             main.py:189\n",
            "2025-04-06 16:59:35 INFO     got prompt                                                server.py:627\n",
            "                    ERROR    Failed to validate prompt for output 130:              execution.py:832\n",
            "                    ERROR    * AddLabel 80:                                         execution.py:852\n",
            "                    ERROR      - Required input is missing: image                   execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 46:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 46:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 61:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 61:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 66:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 66:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "queue_counter: 0\n",
            "additional_args: \n",
            "2025-04-06 16:59:35 INFO     Checking the state dict: Diffusers or BFL, dev or      flux_utils.py:40\n",
            "                             schnell                                                                \n",
            "                    INFO     t5xxl_max_token_length: 512             flux_train_network_comfy.py:111\n",
            "2025-04-06 16:59:36 INFO     Loading dataset config from [[datasets]]           train_network.py:342\n",
            "                             resolution = [ 512, 512,]                                              \n",
            "                             batch_size = 1                                                         \n",
            "                             enable_bucket = true                                                   \n",
            "                             bucket_no_upscale = false                                              \n",
            "                             min_bucket_reso = 256                                                  \n",
            "                             max_bucket_reso = 1024                                                 \n",
            "                             [[datasets.subsets]]                                                   \n",
            "                             image_dir = \"/content/drive/MyDrive/AI/小恩\"                           \n",
            "                             class_tokens = \"akihikoyoshida\"                                        \n",
            "                             num_repeats = 1                                                        \n",
            "                                                                                                    \n",
            "                                                                                                    \n",
            "                             [general]                                                              \n",
            "                             shuffle_caption = false                                                \n",
            "                             caption_extension = \".txt\"                                             \n",
            "                             keep_tokens_separator = \"|||\"                                          \n",
            "                             caption_dropout_rate = 0.0                                             \n",
            "                             color_aug = false                                                      \n",
            "                             flip_aug = false                                                       \n",
            "                                                                                                    \n",
            "                    INFO     prepare images.                                      train_util.py:1955\n",
            "                    INFO     get image size from name of cache files              train_util.py:1864\n",
            "100% 54/54 [00:00<00:00, 39791.36it/s]\n",
            "                    INFO     set image size from cache files: 0/54                train_util.py:1894\n",
            "                    INFO     found directory /content/drive/MyDrive/AI/小恩       train_util.py:1896\n",
            "                             contains 54 image files                                                \n",
            "read caption: 100% 54/54 [00:00<00:00, 2962.70it/s]\n",
            "2025-04-06 16:59:37 INFO     Found captions for 0 images.                         train_util.py:1926\n",
            "                    WARNING  No caption file found for 54 images. Training will   train_util.py:1933\n",
            "                             continue without captions for these images. If class                   \n",
            "                             token exists, it will be used. /                                       \n",
            "                             54枚の画像にキャプションファイルが見つかりませんでし                   \n",
            "                             た。これらの画像についてはキャプションなしで学習を続                   \n",
            "                             行します。class                                                        \n",
            "                             tokenが存在する場合はそれを使います。                                  \n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/02.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/03.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/04.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/05.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/06.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/07.jpg... and 49 more train_util.py:1938\n",
            "                    INFO     54 train images with repeating.                      train_util.py:1996\n",
            "                    INFO     0 reg images.                                        train_util.py:1999\n",
            "                    WARNING  no regularization images /                           train_util.py:2004\n",
            "                             正則化画像が見つかりませんでした                                       \n",
            "                    INFO     [Dataset 0]                                          config_util.py:567\n",
            "                               batch_size: 1                                                        \n",
            "                               resolution: (512, 512)                                               \n",
            "                               enable_bucket: True                                                  \n",
            "                               network_multiplier: 1.0                                              \n",
            "                               min_bucket_reso: 256                                                 \n",
            "                               max_bucket_reso: 1024                                                \n",
            "                               bucket_reso_steps: 64                                                \n",
            "                               bucket_no_upscale: False                                             \n",
            "                                                                                                    \n",
            "                               [Subset 0 of Dataset 0]                                              \n",
            "                                 image_dir: \"/content/drive/MyDrive/AI/小恩\"                        \n",
            "                                 image_count: 54                                                    \n",
            "                                 num_repeats: 1                                                     \n",
            "                                 shuffle_caption: False                                             \n",
            "                                 keep_tokens: 0                                                     \n",
            "                                 keep_tokens_separator: |||                                         \n",
            "                                 caption_separator: ,                                               \n",
            "                                 secondary_separator: None                                          \n",
            "                                 enable_wildcard: False                                             \n",
            "                                 caption_dropout_rate: 0.0                                          \n",
            "                                 caption_dropout_every_n_epochs: 0                                  \n",
            "                                 caption_tag_dropout_rate: 0.0                                      \n",
            "                                 caption_prefix: None                                               \n",
            "                                 caption_suffix: None                                               \n",
            "                                 color_aug: False                                                   \n",
            "                                 flip_aug: False                                                    \n",
            "                                 face_crop_aug_range: None                                          \n",
            "                                 random_crop: False                                                 \n",
            "                                 token_warmup_min: 1                                                \n",
            "                                 token_warmup_step: 0.0                                             \n",
            "                                 alpha_mask: False                                                  \n",
            "                                 custom_attributes: {}                                              \n",
            "                                 is_reg: False                                                      \n",
            "                                 class_tokens: akihikoyoshida                                       \n",
            "                                 caption_extension: .txt                                            \n",
            "                                                                                                    \n",
            "                                                                                                    \n",
            "                    INFO     [Dataset 0]                                          config_util.py:573\n",
            "                    INFO     loading image sizes.                                  train_util.py:918\n",
            "100% 54/54 [00:00<00:00, 364.09it/s]\n",
            "                    INFO     make buckets                                          train_util.py:924\n",
            "                    INFO     number of images (including repeats) /                train_util.py:970\n",
            "                             各bucketの画像枚数（繰り返し回数を含む）                               \n",
            "                    INFO     bucket 0: resolution (448, 576), count: 48            train_util.py:975\n",
            "                    INFO     bucket 1: resolution (576, 448), count: 6             train_util.py:975\n",
            "                    INFO     mean ar error (without repeats): 0.029987980283698975 train_util.py:980\n",
            "                    INFO     preparing accelerator                              train_network.py:407\n",
            "                    ERROR    !!! Exception during processing !!! AcceleratorState   execution.py:396\n",
            "                             has already been initialized and cannot be changed,                    \n",
            "                             restart your runtime completely and pass                               \n",
            "                             `mixed_precision='fp16'` to `Accelerator()`.                           \n",
            "                    ERROR    Traceback (most recent call last):                     execution.py:397\n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 327, in execute                                                   \n",
            "                                 output_data, output_ui, has_subgraph =                             \n",
            "                             get_output_data(obj, input_data_all,                                   \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                                        ^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                              \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 202, in get_output_data                                           \n",
            "                                 return_values = _map_node_over_list(obj,                           \n",
            "                             input_data_all, obj.FUNCTION, allow_interrupt=True,                    \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^                                                                   \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 174, in _map_node_over_list                                       \n",
            "                                 process_inputs(input_dict, i)                                      \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 163, in process_inputs                                            \n",
            "                                 results.append(getattr(obj, func)(**inputs))                       \n",
            "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                        \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/nodes.py\", line 631, in init_training                       \n",
            "                                 training_loop = network_trainer.init_train(args)                   \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                   \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/train_network.py\", line 408, in init_train                  \n",
            "                                 accelerator = train_util.prepare_accelerator(args)                 \n",
            "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/library/train_util.py\", line 5181, in                       \n",
            "                             prepare_accelerator                                                    \n",
            "                                 accelerator = Accelerator(                                         \n",
            "                                               ^^^^^^^^^^^^                                         \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/accelerate/ac                 \n",
            "                             celerator.py\", line 453, in __init__                                   \n",
            "                                 self.state = AcceleratorState(                                     \n",
            "                                              ^^^^^^^^^^^^^^^^^                                     \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/accelerate/st                 \n",
            "                             ate.py\", line 898, in __init__                                         \n",
            "                                 self._check_initialized(mixed_precision, cpu)                      \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/accelerate/st                 \n",
            "                             ate.py\", line 1012, in _check_initialized                              \n",
            "                                 raise                                                              \n",
            "                             ValueError(err.format(flag=f\"mixed_precision='{mixed_p                 \n",
            "                             recision}'\"))                                                          \n",
            "                             ValueError: AcceleratorState has already been                          \n",
            "                             initialized and cannot be changed, restart your                        \n",
            "                             runtime completely and pass `mixed_precision='fp16'`                   \n",
            "                             to `Accelerator()`.                                                    \n",
            "                                                                                                    \n",
            "                    INFO     Prompt executed in 1.43 seconds                             main.py:189\n",
            "2025-04-06 16:59:47 INFO     got prompt                                                server.py:627\n",
            "                    ERROR    Failed to validate prompt for output 130:              execution.py:832\n",
            "                    ERROR    * AddLabel 80:                                         execution.py:852\n",
            "                    ERROR      - Required input is missing: image                   execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 46:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 46:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 61:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 61:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 66:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 66:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "queue_counter: 0\n",
            "additional_args: \n",
            "2025-04-06 16:59:47 INFO     Checking the state dict: Diffusers or BFL, dev or      flux_utils.py:40\n",
            "                             schnell                                                                \n",
            "                    INFO     t5xxl_max_token_length: 512             flux_train_network_comfy.py:111\n",
            "2025-04-06 16:59:48 INFO     Loading dataset config from [[datasets]]           train_network.py:342\n",
            "                             resolution = [ 512, 512,]                                              \n",
            "                             batch_size = 1                                                         \n",
            "                             enable_bucket = true                                                   \n",
            "                             bucket_no_upscale = false                                              \n",
            "                             min_bucket_reso = 256                                                  \n",
            "                             max_bucket_reso = 1024                                                 \n",
            "                             [[datasets.subsets]]                                                   \n",
            "                             image_dir = \"/content/drive/MyDrive/AI/小恩\"                           \n",
            "                             class_tokens = \"akihikoyoshida\"                                        \n",
            "                             num_repeats = 1                                                        \n",
            "                                                                                                    \n",
            "                                                                                                    \n",
            "                             [general]                                                              \n",
            "                             shuffle_caption = false                                                \n",
            "                             caption_extension = \".txt\"                                             \n",
            "                             keep_tokens_separator = \"|||\"                                          \n",
            "                             caption_dropout_rate = 0.0                                             \n",
            "                             color_aug = false                                                      \n",
            "                             flip_aug = false                                                       \n",
            "                                                                                                    \n",
            "                    INFO     prepare images.                                      train_util.py:1955\n",
            "                    INFO     get image size from name of cache files              train_util.py:1864\n",
            "100% 54/54 [00:00<00:00, 19148.83it/s]\n",
            "                    INFO     set image size from cache files: 0/54                train_util.py:1894\n",
            "                    INFO     found directory /content/drive/MyDrive/AI/小恩       train_util.py:1896\n",
            "                             contains 54 image files                                                \n",
            "read caption: 100% 54/54 [00:00<00:00, 1999.53it/s]\n",
            "2025-04-06 16:59:49 INFO     Found captions for 0 images.                         train_util.py:1926\n",
            "                    WARNING  No caption file found for 54 images. Training will   train_util.py:1933\n",
            "                             continue without captions for these images. If class                   \n",
            "                             token exists, it will be used. /                                       \n",
            "                             54枚の画像にキャプションファイルが見つかりませんでし                   \n",
            "                             た。これらの画像についてはキャプションなしで学習を続                   \n",
            "                             行します。class                                                        \n",
            "                             tokenが存在する場合はそれを使います。                                  \n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/02.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/03.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/04.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/05.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/06.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/07.jpg... and 49 more train_util.py:1938\n",
            "                    INFO     54 train images with repeating.                      train_util.py:1996\n",
            "                    INFO     0 reg images.                                        train_util.py:1999\n",
            "                    WARNING  no regularization images /                           train_util.py:2004\n",
            "                             正則化画像が見つかりませんでした                                       \n",
            "                    INFO     [Dataset 0]                                          config_util.py:567\n",
            "                               batch_size: 1                                                        \n",
            "                               resolution: (512, 512)                                               \n",
            "                               enable_bucket: True                                                  \n",
            "                               network_multiplier: 1.0                                              \n",
            "                               min_bucket_reso: 256                                                 \n",
            "                               max_bucket_reso: 1024                                                \n",
            "                               bucket_reso_steps: 64                                                \n",
            "                               bucket_no_upscale: False                                             \n",
            "                                                                                                    \n",
            "                               [Subset 0 of Dataset 0]                                              \n",
            "                                 image_dir: \"/content/drive/MyDrive/AI/小恩\"                        \n",
            "                                 image_count: 54                                                    \n",
            "                                 num_repeats: 1                                                     \n",
            "                                 shuffle_caption: False                                             \n",
            "                                 keep_tokens: 0                                                     \n",
            "                                 keep_tokens_separator: |||                                         \n",
            "                                 caption_separator: ,                                               \n",
            "                                 secondary_separator: None                                          \n",
            "                                 enable_wildcard: False                                             \n",
            "                                 caption_dropout_rate: 0.0                                          \n",
            "                                 caption_dropout_every_n_epochs: 0                                  \n",
            "                                 caption_tag_dropout_rate: 0.0                                      \n",
            "                                 caption_prefix: None                                               \n",
            "                                 caption_suffix: None                                               \n",
            "                                 color_aug: False                                                   \n",
            "                                 flip_aug: False                                                    \n",
            "                                 face_crop_aug_range: None                                          \n",
            "                                 random_crop: False                                                 \n",
            "                                 token_warmup_min: 1                                                \n",
            "                                 token_warmup_step: 0.0                                             \n",
            "                                 alpha_mask: False                                                  \n",
            "                                 custom_attributes: {}                                              \n",
            "                                 is_reg: False                                                      \n",
            "                                 class_tokens: akihikoyoshida                                       \n",
            "                                 caption_extension: .txt                                            \n",
            "                                                                                                    \n",
            "                                                                                                    \n",
            "                    INFO     [Dataset 0]                                          config_util.py:573\n",
            "                    INFO     loading image sizes.                                  train_util.py:918\n",
            "100% 54/54 [00:00<00:00, 461.10it/s]\n",
            "                    INFO     make buckets                                          train_util.py:924\n",
            "                    INFO     number of images (including repeats) /                train_util.py:970\n",
            "                             各bucketの画像枚数（繰り返し回数を含む）                               \n",
            "                    INFO     bucket 0: resolution (448, 576), count: 48            train_util.py:975\n",
            "                    INFO     bucket 1: resolution (576, 448), count: 6             train_util.py:975\n",
            "                    INFO     mean ar error (without repeats): 0.029987980283698975 train_util.py:980\n",
            "                    INFO     preparing accelerator                              train_network.py:407\n",
            "accelerator device: cuda\n",
            "                    INFO     Checking the state dict: Diffusers or BFL, dev or      flux_utils.py:40\n",
            "                             schnell                                                                \n",
            "                    INFO     Building Flux model dev from BFL checkpoint            flux_utils.py:97\n",
            "                    INFO     Loading state dict from                               flux_utils.py:114\n",
            "                             /content/drive/MyDrive/ComfyUI/models/unet/flux1-dev-                  \n",
            "                             fp8.safetensors                                                        \n",
            "                    INFO     Loaded Flux: <All keys matched successfully>          flux_utils.py:132\n",
            "                    INFO     Loaded torch.float8_e4m3fn FLUX model    flux_train_network_comfy.py:69\n",
            "                    INFO     Building CLIP-L                                       flux_utils.py:158\n",
            "                    INFO     Loading state dict from                               flux_utils.py:254\n",
            "                             /content/drive/MyDrive/ComfyUI/models/text_encoders/c                  \n",
            "                             lip_l.safetensors                                                      \n",
            "                    INFO     Loaded CLIP-L: <All keys matched successfully>        flux_utils.py:257\n",
            "2025-04-06 16:59:50 INFO     Loading state dict from                               flux_utils.py:309\n",
            "                             /content/drive/MyDrive/ComfyUI/models/text_encoders/t                  \n",
            "                             5xxl_fp8_e4m3fn.safetensors                                            \n",
            "                    INFO     Loaded T5xxl: <All keys matched successfully>         flux_utils.py:312\n",
            "                    INFO     Loaded fp8 T5XXL model                   flux_train_network_comfy.py:94\n",
            "                    INFO     Building AutoEncoder                                  flux_utils.py:139\n",
            "                    INFO     Loading state dict from                               flux_utils.py:144\n",
            "                             /content/drive/MyDrive/ComfyUI/models/vae/ae.safetens                  \n",
            "                             ors                                                                    \n",
            "                    INFO     Loaded AE:                                            flux_utils.py:147\n",
            "                             _IncompatibleKeys(missing_keys=['encoder.conv_in.weig                  \n",
            "                             ht', 'encoder.conv_in.bias',                                           \n",
            "                             'encoder.down.0.block.0.norm1.weight',                                 \n",
            "                             'encoder.down.0.block.0.norm1.bias',                                   \n",
            "                             'encoder.down.0.block.0.conv1.weight',                                 \n",
            "                             'encoder.down.0.block.0.conv1.bias',                                   \n",
            "                             'encoder.down.0.block.0.norm2.weight',                                 \n",
            "                             'encoder.down.0.block.0.norm2.bias',                                   \n",
            "                             'encoder.down.0.block.0.conv2.weight',                                 \n",
            "                             'encoder.down.0.block.0.conv2.bias',                                   \n",
            "                             'encoder.down.0.block.1.norm1.weight',                                 \n",
            "                             'encoder.down.0.block.1.norm1.bias',                                   \n",
            "                             'encoder.down.0.block.1.conv1.weight',                                 \n",
            "                             'encoder.down.0.block.1.conv1.bias',                                   \n",
            "                             'encoder.down.0.block.1.norm2.weight',                                 \n",
            "                             'encoder.down.0.block.1.norm2.bias',                                   \n",
            "                             'encoder.down.0.block.1.conv2.weight',                                 \n",
            "                             'encoder.down.0.block.1.conv2.bias',                                   \n",
            "                             'encoder.down.0.downsample.conv.weight',                               \n",
            "                             'encoder.down.0.downsample.conv.bias',                                 \n",
            "                             'encoder.down.1.block.0.norm1.weight',                                 \n",
            "                             'encoder.down.1.block.0.norm1.bias',                                   \n",
            "                             'encoder.down.1.block.0.conv1.weight',                                 \n",
            "                             'encoder.down.1.block.0.conv1.bias',                                   \n",
            "                             'encoder.down.1.block.0.norm2.weight',                                 \n",
            "                             'encoder.down.1.block.0.norm2.bias',                                   \n",
            "                             'encoder.down.1.block.0.conv2.weight',                                 \n",
            "                             'encoder.down.1.block.0.conv2.bias',                                   \n",
            "                             'encoder.down.1.block.0.nin_shortcut.weight',                          \n",
            "                             'encoder.down.1.block.0.nin_shortcut.bias',                            \n",
            "                             'encoder.down.1.block.1.norm1.weight',                                 \n",
            "                             'encoder.down.1.block.1.norm1.bias',                                   \n",
            "                             'encoder.down.1.block.1.conv1.weight',                                 \n",
            "                             'encoder.down.1.block.1.conv1.bias',                                   \n",
            "                             'encoder.down.1.block.1.norm2.weight',                                 \n",
            "                             'encoder.down.1.block.1.norm2.bias',                                   \n",
            "                             'encoder.down.1.block.1.conv2.weight',                                 \n",
            "                             'encoder.down.1.block.1.conv2.bias',                                   \n",
            "                             'encoder.down.1.downsample.conv.weight',                               \n",
            "                             'encoder.down.1.downsample.conv.bias',                                 \n",
            "                             'encoder.down.2.block.0.norm1.weight',                                 \n",
            "                             'encoder.down.2.block.0.norm1.bias',                                   \n",
            "                             'encoder.down.2.block.0.conv1.weight',                                 \n",
            "                             'encoder.down.2.block.0.conv1.bias',                                   \n",
            "                             'encoder.down.2.block.0.norm2.weight',                                 \n",
            "                             'encoder.down.2.block.0.norm2.bias',                                   \n",
            "                             'encoder.down.2.block.0.conv2.weight',                                 \n",
            "                             'encoder.down.2.block.0.conv2.bias',                                   \n",
            "                             'encoder.down.2.block.0.nin_shortcut.weight',                          \n",
            "                             'encoder.down.2.block.0.nin_shortcut.bias',                            \n",
            "                             'encoder.down.2.block.1.norm1.weight',                                 \n",
            "                             'encoder.down.2.block.1.norm1.bias',                                   \n",
            "                             'encoder.down.2.block.1.conv1.weight',                                 \n",
            "                             'encoder.down.2.block.1.conv1.bias',                                   \n",
            "                             'encoder.down.2.block.1.norm2.weight',                                 \n",
            "                             'encoder.down.2.block.1.norm2.bias',                                   \n",
            "                             'encoder.down.2.block.1.conv2.weight',                                 \n",
            "                             'encoder.down.2.block.1.conv2.bias',                                   \n",
            "                             'encoder.down.2.downsample.conv.weight',                               \n",
            "                             'encoder.down.2.downsample.conv.bias',                                 \n",
            "                             'encoder.down.3.block.0.norm1.weight',                                 \n",
            "                             'encoder.down.3.block.0.norm1.bias',                                   \n",
            "                             'encoder.down.3.block.0.conv1.weight',                                 \n",
            "                             'encoder.down.3.block.0.conv1.bias',                                   \n",
            "                             'encoder.down.3.block.0.norm2.weight',                                 \n",
            "                             'encoder.down.3.block.0.norm2.bias',                                   \n",
            "                             'encoder.down.3.block.0.conv2.weight',                                 \n",
            "                             'encoder.down.3.block.0.conv2.bias',                                   \n",
            "                             'encoder.down.3.block.1.norm1.weight',                                 \n",
            "                             'encoder.down.3.block.1.norm1.bias',                                   \n",
            "                             'encoder.down.3.block.1.conv1.weight',                                 \n",
            "                             'encoder.down.3.block.1.conv1.bias',                                   \n",
            "                             'encoder.down.3.block.1.norm2.weight',                                 \n",
            "                             'encoder.down.3.block.1.norm2.bias',                                   \n",
            "                             'encoder.down.3.block.1.conv2.weight',                                 \n",
            "                             'encoder.down.3.block.1.conv2.bias',                                   \n",
            "                             'encoder.mid.block_1.norm1.weight',                                    \n",
            "                             'encoder.mid.block_1.norm1.bias',                                      \n",
            "                             'encoder.mid.block_1.conv1.weight',                                    \n",
            "                             'encoder.mid.block_1.conv1.bias',                                      \n",
            "                             'encoder.mid.block_1.norm2.weight',                                    \n",
            "                             'encoder.mid.block_1.norm2.bias',                                      \n",
            "                             'encoder.mid.block_1.conv2.weight',                                    \n",
            "                             'encoder.mid.block_1.conv2.bias',                                      \n",
            "                             'encoder.mid.attn_1.norm.weight',                                      \n",
            "                             'encoder.mid.attn_1.norm.bias',                                        \n",
            "                             'encoder.mid.attn_1.q.weight',                                         \n",
            "                             'encoder.mid.attn_1.q.bias',                                           \n",
            "                             'encoder.mid.attn_1.k.weight',                                         \n",
            "                             'encoder.mid.attn_1.k.bias',                                           \n",
            "                             'encoder.mid.attn_1.v.weight',                                         \n",
            "                             'encoder.mid.attn_1.v.bias',                                           \n",
            "                             'encoder.mid.attn_1.proj_out.weight',                                  \n",
            "                             'encoder.mid.attn_1.proj_out.bias',                                    \n",
            "                             'encoder.mid.block_2.norm1.weight',                                    \n",
            "                             'encoder.mid.block_2.norm1.bias',                                      \n",
            "                             'encoder.mid.block_2.conv1.weight',                                    \n",
            "                             'encoder.mid.block_2.conv1.bias',                                      \n",
            "                             'encoder.mid.block_2.norm2.weight',                                    \n",
            "                             'encoder.mid.block_2.norm2.bias',                                      \n",
            "                             'encoder.mid.block_2.conv2.weight',                                    \n",
            "                             'encoder.mid.block_2.conv2.bias',                                      \n",
            "                             'encoder.norm_out.weight', 'encoder.norm_out.bias',                    \n",
            "                             'encoder.conv_out.weight', 'encoder.conv_out.bias',                    \n",
            "                             'decoder.conv_in.weight', 'decoder.conv_in.bias',                      \n",
            "                             'decoder.mid.block_1.norm1.weight',                                    \n",
            "                             'decoder.mid.block_1.norm1.bias',                                      \n",
            "                             'decoder.mid.block_1.conv1.weight',                                    \n",
            "                             'decoder.mid.block_1.conv1.bias',                                      \n",
            "                             'decoder.mid.block_1.norm2.weight',                                    \n",
            "                             'decoder.mid.block_1.norm2.bias',                                      \n",
            "                             'decoder.mid.block_1.conv2.weight',                                    \n",
            "                             'decoder.mid.block_1.conv2.bias',                                      \n",
            "                             'decoder.mid.attn_1.norm.weight',                                      \n",
            "                             'decoder.mid.attn_1.norm.bias',                                        \n",
            "                             'decoder.mid.attn_1.q.weight',                                         \n",
            "                             'decoder.mid.attn_1.q.bias',                                           \n",
            "                             'decoder.mid.attn_1.k.weight',                                         \n",
            "                             'decoder.mid.attn_1.k.bias',                                           \n",
            "                             'decoder.mid.attn_1.v.weight',                                         \n",
            "                             'decoder.mid.attn_1.v.bias',                                           \n",
            "                             'decoder.mid.attn_1.proj_out.weight',                                  \n",
            "                             'decoder.mid.attn_1.proj_out.bias',                                    \n",
            "                             'decoder.mid.block_2.norm1.weight',                                    \n",
            "                             'decoder.mid.block_2.norm1.bias',                                      \n",
            "                             'decoder.mid.block_2.conv1.weight',                                    \n",
            "                             'decoder.mid.block_2.conv1.bias',                                      \n",
            "                             'decoder.mid.block_2.norm2.weight',                                    \n",
            "                             'decoder.mid.block_2.norm2.bias',                                      \n",
            "                             'decoder.mid.block_2.conv2.weight',                                    \n",
            "                             'decoder.mid.block_2.conv2.bias',                                      \n",
            "                             'decoder.up.0.block.0.norm1.weight',                                   \n",
            "                             'decoder.up.0.block.0.norm1.bias',                                     \n",
            "                             'decoder.up.0.block.0.conv1.weight',                                   \n",
            "                             'decoder.up.0.block.0.conv1.bias',                                     \n",
            "                             'decoder.up.0.block.0.norm2.weight',                                   \n",
            "                             'decoder.up.0.block.0.norm2.bias',                                     \n",
            "                             'decoder.up.0.block.0.conv2.weight',                                   \n",
            "                             'decoder.up.0.block.0.conv2.bias',                                     \n",
            "                             'decoder.up.0.block.0.nin_shortcut.weight',                            \n",
            "                             'decoder.up.0.block.0.nin_shortcut.bias',                              \n",
            "                             'decoder.up.0.block.1.norm1.weight',                                   \n",
            "                             'decoder.up.0.block.1.norm1.bias',                                     \n",
            "                             'decoder.up.0.block.1.conv1.weight',                                   \n",
            "                             'decoder.up.0.block.1.conv1.bias',                                     \n",
            "                             'decoder.up.0.block.1.norm2.weight',                                   \n",
            "                             'decoder.up.0.block.1.norm2.bias',                                     \n",
            "                             'decoder.up.0.block.1.conv2.weight',                                   \n",
            "                             'decoder.up.0.block.1.conv2.bias',                                     \n",
            "                             'decoder.up.0.block.2.norm1.weight',                                   \n",
            "                             'decoder.up.0.block.2.norm1.bias',                                     \n",
            "                             'decoder.up.0.block.2.conv1.weight',                                   \n",
            "                             'decoder.up.0.block.2.conv1.bias',                                     \n",
            "                             'decoder.up.0.block.2.norm2.weight',                                   \n",
            "                             'decoder.up.0.block.2.norm2.bias',                                     \n",
            "                             'decoder.up.0.block.2.conv2.weight',                                   \n",
            "                             'decoder.up.0.block.2.conv2.bias',                                     \n",
            "                             'decoder.up.1.block.0.norm1.weight',                                   \n",
            "                             'decoder.up.1.block.0.norm1.bias',                                     \n",
            "                             'decoder.up.1.block.0.conv1.weight',                                   \n",
            "                             'decoder.up.1.block.0.conv1.bias',                                     \n",
            "                             'decoder.up.1.block.0.norm2.weight',                                   \n",
            "                             'decoder.up.1.block.0.norm2.bias',                                     \n",
            "                             'decoder.up.1.block.0.conv2.weight',                                   \n",
            "                             'decoder.up.1.block.0.conv2.bias',                                     \n",
            "                             'decoder.up.1.block.0.nin_shortcut.weight',                            \n",
            "                             'decoder.up.1.block.0.nin_shortcut.bias',                              \n",
            "                             'decoder.up.1.block.1.norm1.weight',                                   \n",
            "                             'decoder.up.1.block.1.norm1.bias',                                     \n",
            "                             'decoder.up.1.block.1.conv1.weight',                                   \n",
            "                             'decoder.up.1.block.1.conv1.bias',                                     \n",
            "                             'decoder.up.1.block.1.norm2.weight',                                   \n",
            "                             'decoder.up.1.block.1.norm2.bias',                                     \n",
            "                             'decoder.up.1.block.1.conv2.weight',                                   \n",
            "                             'decoder.up.1.block.1.conv2.bias',                                     \n",
            "                             'decoder.up.1.block.2.norm1.weight',                                   \n",
            "                             'decoder.up.1.block.2.norm1.bias',                                     \n",
            "                             'decoder.up.1.block.2.conv1.weight',                                   \n",
            "                             'decoder.up.1.block.2.conv1.bias',                                     \n",
            "                             'decoder.up.1.block.2.norm2.weight',                                   \n",
            "                             'decoder.up.1.block.2.norm2.bias',                                     \n",
            "                             'decoder.up.1.block.2.conv2.weight',                                   \n",
            "                             'decoder.up.1.block.2.conv2.bias',                                     \n",
            "                             'decoder.up.1.upsample.conv.weight',                                   \n",
            "                             'decoder.up.1.upsample.conv.bias',                                     \n",
            "                             'decoder.up.2.block.0.norm1.weight',                                   \n",
            "                             'decoder.up.2.block.0.norm1.bias',                                     \n",
            "                             'decoder.up.2.block.0.conv1.weight',                                   \n",
            "                             'decoder.up.2.block.0.conv1.bias',                                     \n",
            "                             'decoder.up.2.block.0.norm2.weight',                                   \n",
            "                             'decoder.up.2.block.0.norm2.bias',                                     \n",
            "                             'decoder.up.2.block.0.conv2.weight',                                   \n",
            "                             'decoder.up.2.block.0.conv2.bias',                                     \n",
            "                             'decoder.up.2.block.1.norm1.weight',                                   \n",
            "                             'decoder.up.2.block.1.norm1.bias',                                     \n",
            "                             'decoder.up.2.block.1.conv1.weight',                                   \n",
            "                             'decoder.up.2.block.1.conv1.bias',                                     \n",
            "                             'decoder.up.2.block.1.norm2.weight',                                   \n",
            "                             'decoder.up.2.block.1.norm2.bias',                                     \n",
            "                             'decoder.up.2.block.1.conv2.weight',                                   \n",
            "                             'decoder.up.2.block.1.conv2.bias',                                     \n",
            "                             'decoder.up.2.block.2.norm1.weight',                                   \n",
            "                             'decoder.up.2.block.2.norm1.bias',                                     \n",
            "                             'decoder.up.2.block.2.conv1.weight',                                   \n",
            "                             'decoder.up.2.block.2.conv1.bias',                                     \n",
            "                             'decoder.up.2.block.2.norm2.weight',                                   \n",
            "                             'decoder.up.2.block.2.norm2.bias',                                     \n",
            "                             'decoder.up.2.block.2.conv2.weight',                                   \n",
            "                             'decoder.up.2.block.2.conv2.bias',                                     \n",
            "                             'decoder.up.2.upsample.conv.weight',                                   \n",
            "                             'decoder.up.2.upsample.conv.bias',                                     \n",
            "                             'decoder.up.3.block.0.norm1.weight',                                   \n",
            "                             'decoder.up.3.block.0.norm1.bias',                                     \n",
            "                             'decoder.up.3.block.0.conv1.weight',                                   \n",
            "                             'decoder.up.3.block.0.conv1.bias',                                     \n",
            "                             'decoder.up.3.block.0.norm2.weight',                                   \n",
            "                             'decoder.up.3.block.0.norm2.bias',                                     \n",
            "                             'decoder.up.3.block.0.conv2.weight',                                   \n",
            "                             'decoder.up.3.block.0.conv2.bias',                                     \n",
            "                             'decoder.up.3.block.1.norm1.weight',                                   \n",
            "                             'decoder.up.3.block.1.norm1.bias',                                     \n",
            "                             'decoder.up.3.block.1.conv1.weight',                                   \n",
            "                             'decoder.up.3.block.1.conv1.bias',                                     \n",
            "                             'decoder.up.3.block.1.norm2.weight',                                   \n",
            "                             'decoder.up.3.block.1.norm2.bias',                                     \n",
            "                             'decoder.up.3.block.1.conv2.weight',                                   \n",
            "                             'decoder.up.3.block.1.conv2.bias',                                     \n",
            "                             'decoder.up.3.block.2.norm1.weight',                                   \n",
            "                             'decoder.up.3.block.2.norm1.bias',                                     \n",
            "                             'decoder.up.3.block.2.conv1.weight',                                   \n",
            "                             'decoder.up.3.block.2.conv1.bias',                                     \n",
            "                             'decoder.up.3.block.2.norm2.weight',                                   \n",
            "                             'decoder.up.3.block.2.norm2.bias',                                     \n",
            "                             'decoder.up.3.block.2.conv2.weight',                                   \n",
            "                             'decoder.up.3.block.2.conv2.bias',                                     \n",
            "                             'decoder.up.3.upsample.conv.weight',                                   \n",
            "                             'decoder.up.3.upsample.conv.bias',                                     \n",
            "                             'decoder.norm_out.weight', 'decoder.norm_out.bias',                    \n",
            "                             'decoder.conv_out.weight', 'decoder.conv_out.bias'],                   \n",
            "                             unexpected_keys=['lora_te_text_model_encoder_layers_0                  \n",
            "                             _mlp_fc1.alpha',                                                       \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc1.alpha',                  \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc1.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc2.alpha',                  \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc2.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_k_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_k_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_k_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_out_p                  \n",
            "                             roj.alpha',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_out_p                  \n",
            "                             roj.lora_down.weight',                                                 \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_out_p                  \n",
            "                             roj.lora_up.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_q_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_q_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_q_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_v_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_v_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_v_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc1.alpha',                  \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc1.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc2.alpha',                  \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc2.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_k_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_k_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_k_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_out_p                  \n",
            "                             roj.alpha',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_out_p                  \n",
            "                             roj.lora_down.weight',                                                 \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_out_p                  \n",
            "                             roj.lora_up.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_q_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_q_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_q_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_v_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_v_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_v_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_proj_in.alpha',                      \n",
            "                             'lora_unet_mid_block_attentions_0_proj_in.lora_down.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_mid_block_attentions_0_proj_in.lora_up.wei                  \n",
            "                             ght',                                                                  \n",
            "                             'lora_unet_mid_block_attentions_0_proj_out.alpha',                     \n",
            "                             'lora_unet_mid_block_attentions_0_proj_out.lora_down.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_mid_block_attentions_0_proj_out.lora_up.we                  \n",
            "                             ight',                                                                 \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_k.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_k.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_k.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_out_0.alpha',                                               \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_out_0.lora_down.weight',                                    \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_out_0.lora_up.weight',                                      \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_q.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_q.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_q.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_v.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_v.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_v.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_k.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_k.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_k.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_out_0.alpha',                                               \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_out_0.lora_down.weight',                                    \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_out_0.lora_up.weight',                                      \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_q.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_q.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_q.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_v.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_v.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_v.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_0_proj.alpha',                                                \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_0_proj.lora_down.weight',                                     \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_0_proj.lora_up.weight',                                       \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_2.alpha',                                                     \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_2.lora_down.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_2.lora_up.weight',                                            \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight'])                                         \n",
            "import network module: .networks.lora_flux\n",
            "                    ERROR    !!! Exception during processing !!! Cannot copy out of execution.py:396\n",
            "                             meta tensor; no data! Please use                                       \n",
            "                             torch.nn.Module.to_empty() instead of                                  \n",
            "                             torch.nn.Module.to() when moving module from meta to a                 \n",
            "                             different device.                                                      \n",
            "                    ERROR    Traceback (most recent call last):                     execution.py:397\n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 327, in execute                                                   \n",
            "                                 output_data, output_ui, has_subgraph =                             \n",
            "                             get_output_data(obj, input_data_all,                                   \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                                        ^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                              \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 202, in get_output_data                                           \n",
            "                                 return_values = _map_node_over_list(obj,                           \n",
            "                             input_data_all, obj.FUNCTION, allow_interrupt=True,                    \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^                                                                   \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 174, in _map_node_over_list                                       \n",
            "                                 process_inputs(input_dict, i)                                      \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 163, in process_inputs                                            \n",
            "                                 results.append(getattr(obj, func)(**inputs))                       \n",
            "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                        \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/nodes.py\", line 631, in init_training                       \n",
            "                                 training_loop = network_trainer.init_train(args)                   \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                   \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/train_network.py\", line 449, in init_train                  \n",
            "                                 vae.to(accelerator.device, dtype=vae_dtype)                        \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 1343, in to                                       \n",
            "                                 return self._apply(convert)                                        \n",
            "                                        ^^^^^^^^^^^^^^^^^^^^                                        \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 903, in _apply                                    \n",
            "                                 module._apply(fn)                                                  \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 903, in _apply                                    \n",
            "                                 module._apply(fn)                                                  \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 930, in _apply                                    \n",
            "                                 param_applied = fn(param)                                          \n",
            "                                                 ^^^^^^^^^                                          \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 1336, in convert                                  \n",
            "                                 raise NotImplementedError(                                         \n",
            "                             NotImplementedError: Cannot copy out of meta tensor;                   \n",
            "                             no data! Please use torch.nn.Module.to_empty() instead                 \n",
            "                             of torch.nn.Module.to() when moving module from meta                   \n",
            "                             to a different device.                                                 \n",
            "                                                                                                    \n",
            "                    INFO     Prompt executed in 2.95 seconds                             main.py:189\n",
            "2025-04-06 17:02:08 INFO     got prompt                                                server.py:627\n",
            "                    ERROR    Failed to validate prompt for output 130:              execution.py:832\n",
            "                    ERROR    * AddLabel 80:                                         execution.py:852\n",
            "                    ERROR      - Required input is missing: image                   execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 46:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 46:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 61:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 61:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 66:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 66:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "queue_counter: 0\n",
            "additional_args: \n",
            "2025-04-06 17:02:08 INFO     Checking the state dict: Diffusers or BFL, dev or      flux_utils.py:40\n",
            "                             schnell                                                                \n",
            "                    INFO     t5xxl_max_token_length: 512             flux_train_network_comfy.py:111\n",
            "2025-04-06 17:02:09 INFO     Loading dataset config from [[datasets]]           train_network.py:342\n",
            "                             resolution = [ 512, 512,]                                              \n",
            "                             batch_size = 1                                                         \n",
            "                             enable_bucket = true                                                   \n",
            "                             bucket_no_upscale = false                                              \n",
            "                             min_bucket_reso = 256                                                  \n",
            "                             max_bucket_reso = 1024                                                 \n",
            "                             [[datasets.subsets]]                                                   \n",
            "                             image_dir = \"/content/drive/MyDrive/AI/小恩\"                           \n",
            "                             class_tokens = \"akihikoyoshida\"                                        \n",
            "                             num_repeats = 1                                                        \n",
            "                                                                                                    \n",
            "                                                                                                    \n",
            "                             [general]                                                              \n",
            "                             shuffle_caption = false                                                \n",
            "                             caption_extension = \".txt\"                                             \n",
            "                             keep_tokens_separator = \"|||\"                                          \n",
            "                             caption_dropout_rate = 0.0                                             \n",
            "                             color_aug = false                                                      \n",
            "                             flip_aug = false                                                       \n",
            "                                                                                                    \n",
            "                    INFO     prepare images.                                      train_util.py:1955\n",
            "                    INFO     get image size from name of cache files              train_util.py:1864\n",
            "100% 54/54 [00:00<00:00, 34722.12it/s]\n",
            "                    INFO     set image size from cache files: 0/54                train_util.py:1894\n",
            "                    INFO     found directory /content/drive/MyDrive/AI/小恩       train_util.py:1896\n",
            "                             contains 54 image files                                                \n",
            "read caption: 100% 54/54 [00:00<00:00, 3410.26it/s]\n",
            "                    INFO     Found captions for 0 images.                         train_util.py:1926\n",
            "                    WARNING  No caption file found for 54 images. Training will   train_util.py:1933\n",
            "                             continue without captions for these images. If class                   \n",
            "                             token exists, it will be used. /                                       \n",
            "                             54枚の画像にキャプションファイルが見つかりませんでし                   \n",
            "                             た。これらの画像についてはキャプションなしで学習を続                   \n",
            "                             行します。class                                                        \n",
            "                             tokenが存在する場合はそれを使います。                                  \n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/02.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/03.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/04.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/05.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/06.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/07.jpg... and 49 more train_util.py:1938\n",
            "                    INFO     54 train images with repeating.                      train_util.py:1996\n",
            "                    INFO     0 reg images.                                        train_util.py:1999\n",
            "                    WARNING  no regularization images /                           train_util.py:2004\n",
            "                             正則化画像が見つかりませんでした                                       \n",
            "                    INFO     [Dataset 0]                                          config_util.py:567\n",
            "                               batch_size: 1                                                        \n",
            "                               resolution: (512, 512)                                               \n",
            "                               enable_bucket: True                                                  \n",
            "                               network_multiplier: 1.0                                              \n",
            "                               min_bucket_reso: 256                                                 \n",
            "                               max_bucket_reso: 1024                                                \n",
            "                               bucket_reso_steps: 64                                                \n",
            "                               bucket_no_upscale: False                                             \n",
            "                                                                                                    \n",
            "                               [Subset 0 of Dataset 0]                                              \n",
            "                                 image_dir: \"/content/drive/MyDrive/AI/小恩\"                        \n",
            "                                 image_count: 54                                                    \n",
            "                                 num_repeats: 1                                                     \n",
            "                                 shuffle_caption: False                                             \n",
            "                                 keep_tokens: 0                                                     \n",
            "                                 keep_tokens_separator: |||                                         \n",
            "                                 caption_separator: ,                                               \n",
            "                                 secondary_separator: None                                          \n",
            "                                 enable_wildcard: False                                             \n",
            "                                 caption_dropout_rate: 0.0                                          \n",
            "                                 caption_dropout_every_n_epochs: 0                                  \n",
            "                                 caption_tag_dropout_rate: 0.0                                      \n",
            "                                 caption_prefix: None                                               \n",
            "                                 caption_suffix: None                                               \n",
            "                                 color_aug: False                                                   \n",
            "                                 flip_aug: False                                                    \n",
            "                                 face_crop_aug_range: None                                          \n",
            "                                 random_crop: False                                                 \n",
            "                                 token_warmup_min: 1                                                \n",
            "                                 token_warmup_step: 0.0                                             \n",
            "                                 alpha_mask: False                                                  \n",
            "                                 custom_attributes: {}                                              \n",
            "                                 is_reg: False                                                      \n",
            "                                 class_tokens: akihikoyoshida                                       \n",
            "                                 caption_extension: .txt                                            \n",
            "                                                                                                    \n",
            "                                                                                                    \n",
            "                    INFO     [Dataset 0]                                          config_util.py:573\n",
            "                    INFO     loading image sizes.                                  train_util.py:918\n",
            "100% 54/54 [00:00<00:00, 371.76it/s]\n",
            "                    INFO     make buckets                                          train_util.py:924\n",
            "                    INFO     number of images (including repeats) /                train_util.py:970\n",
            "                             各bucketの画像枚数（繰り返し回数を含む）                               \n",
            "                    INFO     bucket 0: resolution (448, 576), count: 48            train_util.py:975\n",
            "                    INFO     bucket 1: resolution (576, 448), count: 6             train_util.py:975\n",
            "                    INFO     mean ar error (without repeats): 0.029987980283698975 train_util.py:980\n",
            "                    INFO     preparing accelerator                              train_network.py:407\n",
            "                    ERROR    !!! Exception during processing !!! AcceleratorState   execution.py:396\n",
            "                             has already been initialized and cannot be changed,                    \n",
            "                             restart your runtime completely and pass                               \n",
            "                             `mixed_precision='fp16'` to `Accelerator()`.                           \n",
            "                    ERROR    Traceback (most recent call last):                     execution.py:397\n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 327, in execute                                                   \n",
            "                                 output_data, output_ui, has_subgraph =                             \n",
            "                             get_output_data(obj, input_data_all,                                   \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                                        ^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                              \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 202, in get_output_data                                           \n",
            "                                 return_values = _map_node_over_list(obj,                           \n",
            "                             input_data_all, obj.FUNCTION, allow_interrupt=True,                    \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^                                                                   \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 174, in _map_node_over_list                                       \n",
            "                                 process_inputs(input_dict, i)                                      \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 163, in process_inputs                                            \n",
            "                                 results.append(getattr(obj, func)(**inputs))                       \n",
            "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                        \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/nodes.py\", line 631, in init_training                       \n",
            "                                 training_loop = network_trainer.init_train(args)                   \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                   \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/train_network.py\", line 408, in init_train                  \n",
            "                                 accelerator = train_util.prepare_accelerator(args)                 \n",
            "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/library/train_util.py\", line 5181, in                       \n",
            "                             prepare_accelerator                                                    \n",
            "                                 accelerator = Accelerator(                                         \n",
            "                                               ^^^^^^^^^^^^                                         \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/accelerate/ac                 \n",
            "                             celerator.py\", line 453, in __init__                                   \n",
            "                                 self.state = AcceleratorState(                                     \n",
            "                                              ^^^^^^^^^^^^^^^^^                                     \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/accelerate/st                 \n",
            "                             ate.py\", line 898, in __init__                                         \n",
            "                                 self._check_initialized(mixed_precision, cpu)                      \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/accelerate/st                 \n",
            "                             ate.py\", line 1012, in _check_initialized                              \n",
            "                                 raise                                                              \n",
            "                             ValueError(err.format(flag=f\"mixed_precision='{mixed_p                 \n",
            "                             recision}'\"))                                                          \n",
            "                             ValueError: AcceleratorState has already been                          \n",
            "                             initialized and cannot be changed, restart your                        \n",
            "                             runtime completely and pass `mixed_precision='fp16'`                   \n",
            "                             to `Accelerator()`.                                                    \n",
            "                                                                                                    \n",
            "                    INFO     Prompt executed in 1.31 seconds                             main.py:189\n",
            "2025-04-06 17:02:22 INFO     got prompt                                                server.py:627\n",
            "2025-04-06 17:02:23 ERROR    Failed to validate prompt for output 130:              execution.py:832\n",
            "                    ERROR    * AddLabel 80:                                         execution.py:852\n",
            "                    ERROR      - Required input is missing: image                   execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 46:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 46:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 61:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 61:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "                    ERROR    Failed to validate prompt for output 66:               execution.py:832\n",
            "                    ERROR    * (prompt):                                            execution.py:834\n",
            "                    ERROR      - Required input is missing: images                  execution.py:836\n",
            "                    ERROR    * PreviewImage 66:                                     execution.py:852\n",
            "                    ERROR      - Required input is missing: images                  execution.py:854\n",
            "                    ERROR    Output will be ignored                                 execution.py:856\n",
            "queue_counter: 0\n",
            "additional_args: \n",
            "2025-04-06 17:02:23 INFO     Checking the state dict: Diffusers or BFL, dev or      flux_utils.py:40\n",
            "                             schnell                                                                \n",
            "                    INFO     t5xxl_max_token_length: 512             flux_train_network_comfy.py:111\n",
            "2025-04-06 17:02:24 INFO     Loading dataset config from [[datasets]]           train_network.py:342\n",
            "                             resolution = [ 512, 512,]                                              \n",
            "                             batch_size = 1                                                         \n",
            "                             enable_bucket = true                                                   \n",
            "                             bucket_no_upscale = false                                              \n",
            "                             min_bucket_reso = 256                                                  \n",
            "                             max_bucket_reso = 1024                                                 \n",
            "                             [[datasets.subsets]]                                                   \n",
            "                             image_dir = \"/content/drive/MyDrive/AI/小恩\"                           \n",
            "                             class_tokens = \"akihikoyoshida\"                                        \n",
            "                             num_repeats = 1                                                        \n",
            "                                                                                                    \n",
            "                                                                                                    \n",
            "                             [general]                                                              \n",
            "                             shuffle_caption = false                                                \n",
            "                             caption_extension = \".txt\"                                             \n",
            "                             keep_tokens_separator = \"|||\"                                          \n",
            "                             caption_dropout_rate = 0.0                                             \n",
            "                             color_aug = false                                                      \n",
            "                             flip_aug = false                                                       \n",
            "                                                                                                    \n",
            "                    INFO     prepare images.                                      train_util.py:1955\n",
            "                    INFO     get image size from name of cache files              train_util.py:1864\n",
            "100% 54/54 [00:00<00:00, 43149.63it/s]\n",
            "                    INFO     set image size from cache files: 0/54                train_util.py:1894\n",
            "                    INFO     found directory /content/drive/MyDrive/AI/小恩       train_util.py:1896\n",
            "                             contains 54 image files                                                \n",
            "read caption: 100% 54/54 [00:00<00:00, 1925.07it/s]\n",
            "                    INFO     Found captions for 0 images.                         train_util.py:1926\n",
            "                    WARNING  No caption file found for 54 images. Training will   train_util.py:1933\n",
            "                             continue without captions for these images. If class                   \n",
            "                             token exists, it will be used. /                                       \n",
            "                             54枚の画像にキャプションファイルが見つかりませんでし                   \n",
            "                             た。これらの画像についてはキャプションなしで学習を続                   \n",
            "                             行します。class                                                        \n",
            "                             tokenが存在する場合はそれを使います。                                  \n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/02.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/03.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/04.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/05.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/06.jpg                train_util.py:1940\n",
            "                    WARNING  /content/drive/MyDrive/AI/小恩/07.jpg... and 49 more train_util.py:1938\n",
            "                    INFO     54 train images with repeating.                      train_util.py:1996\n",
            "                    INFO     0 reg images.                                        train_util.py:1999\n",
            "                    WARNING  no regularization images /                           train_util.py:2004\n",
            "                             正則化画像が見つかりませんでした                                       \n",
            "                    INFO     [Dataset 0]                                          config_util.py:567\n",
            "                               batch_size: 1                                                        \n",
            "                               resolution: (512, 512)                                               \n",
            "                               enable_bucket: True                                                  \n",
            "                               network_multiplier: 1.0                                              \n",
            "                               min_bucket_reso: 256                                                 \n",
            "                               max_bucket_reso: 1024                                                \n",
            "                               bucket_reso_steps: 64                                                \n",
            "                               bucket_no_upscale: False                                             \n",
            "                                                                                                    \n",
            "                               [Subset 0 of Dataset 0]                                              \n",
            "                                 image_dir: \"/content/drive/MyDrive/AI/小恩\"                        \n",
            "                                 image_count: 54                                                    \n",
            "                                 num_repeats: 1                                                     \n",
            "                                 shuffle_caption: False                                             \n",
            "                                 keep_tokens: 0                                                     \n",
            "                                 keep_tokens_separator: |||                                         \n",
            "                                 caption_separator: ,                                               \n",
            "                                 secondary_separator: None                                          \n",
            "                                 enable_wildcard: False                                             \n",
            "                                 caption_dropout_rate: 0.0                                          \n",
            "                                 caption_dropout_every_n_epochs: 0                                  \n",
            "                                 caption_tag_dropout_rate: 0.0                                      \n",
            "                                 caption_prefix: None                                               \n",
            "                                 caption_suffix: None                                               \n",
            "                                 color_aug: False                                                   \n",
            "                                 flip_aug: False                                                    \n",
            "                                 face_crop_aug_range: None                                          \n",
            "                                 random_crop: False                                                 \n",
            "                                 token_warmup_min: 1                                                \n",
            "                                 token_warmup_step: 0.0                                             \n",
            "                                 alpha_mask: False                                                  \n",
            "                                 custom_attributes: {}                                              \n",
            "                                 is_reg: False                                                      \n",
            "                                 class_tokens: akihikoyoshida                                       \n",
            "                                 caption_extension: .txt                                            \n",
            "                                                                                                    \n",
            "                                                                                                    \n",
            "                    INFO     [Dataset 0]                                          config_util.py:573\n",
            "                    INFO     loading image sizes.                                  train_util.py:918\n",
            "100% 54/54 [00:00<00:00, 476.16it/s]\n",
            "2025-04-06 17:02:25 INFO     make buckets                                          train_util.py:924\n",
            "                    INFO     number of images (including repeats) /                train_util.py:970\n",
            "                             各bucketの画像枚数（繰り返し回数を含む）                               \n",
            "                    INFO     bucket 0: resolution (448, 576), count: 48            train_util.py:975\n",
            "                    INFO     bucket 1: resolution (576, 448), count: 6             train_util.py:975\n",
            "                    INFO     mean ar error (without repeats): 0.029987980283698975 train_util.py:980\n",
            "                    INFO     preparing accelerator                              train_network.py:407\n",
            "accelerator device: cuda\n",
            "                    INFO     Checking the state dict: Diffusers or BFL, dev or      flux_utils.py:40\n",
            "                             schnell                                                                \n",
            "                    INFO     Building Flux model dev from BFL checkpoint            flux_utils.py:97\n",
            "                    INFO     Loading state dict from                               flux_utils.py:114\n",
            "                             /content/drive/MyDrive/ComfyUI/models/unet/flux1-dev-                  \n",
            "                             fp8.safetensors                                                        \n",
            "                    INFO     Loaded Flux: <All keys matched successfully>          flux_utils.py:132\n",
            "                    INFO     Loaded torch.float8_e4m3fn FLUX model    flux_train_network_comfy.py:69\n",
            "                    INFO     Building CLIP-L                                       flux_utils.py:158\n",
            "                    INFO     Loading state dict from                               flux_utils.py:254\n",
            "                             /content/drive/MyDrive/ComfyUI/models/text_encoders/c                  \n",
            "                             lip_l.safetensors                                                      \n",
            "                    INFO     Loaded CLIP-L: <All keys matched successfully>        flux_utils.py:257\n",
            "                    INFO     Loading state dict from                               flux_utils.py:309\n",
            "                             /content/drive/MyDrive/ComfyUI/models/text_encoders/t                  \n",
            "                             5xxl_fp8_e4m3fn.safetensors                                            \n",
            "2025-04-06 17:02:26 INFO     Loaded T5xxl: <All keys matched successfully>         flux_utils.py:312\n",
            "                    INFO     Loaded fp8 T5XXL model                   flux_train_network_comfy.py:94\n",
            "                    INFO     Building AutoEncoder                                  flux_utils.py:139\n",
            "                    INFO     Loading state dict from                               flux_utils.py:144\n",
            "                             /content/drive/MyDrive/ComfyUI/models/vae/ae.safetens                  \n",
            "                             ors                                                                    \n",
            "                    INFO     Loaded AE:                                            flux_utils.py:147\n",
            "                             _IncompatibleKeys(missing_keys=['encoder.conv_in.weig                  \n",
            "                             ht', 'encoder.conv_in.bias',                                           \n",
            "                             'encoder.down.0.block.0.norm1.weight',                                 \n",
            "                             'encoder.down.0.block.0.norm1.bias',                                   \n",
            "                             'encoder.down.0.block.0.conv1.weight',                                 \n",
            "                             'encoder.down.0.block.0.conv1.bias',                                   \n",
            "                             'encoder.down.0.block.0.norm2.weight',                                 \n",
            "                             'encoder.down.0.block.0.norm2.bias',                                   \n",
            "                             'encoder.down.0.block.0.conv2.weight',                                 \n",
            "                             'encoder.down.0.block.0.conv2.bias',                                   \n",
            "                             'encoder.down.0.block.1.norm1.weight',                                 \n",
            "                             'encoder.down.0.block.1.norm1.bias',                                   \n",
            "                             'encoder.down.0.block.1.conv1.weight',                                 \n",
            "                             'encoder.down.0.block.1.conv1.bias',                                   \n",
            "                             'encoder.down.0.block.1.norm2.weight',                                 \n",
            "                             'encoder.down.0.block.1.norm2.bias',                                   \n",
            "                             'encoder.down.0.block.1.conv2.weight',                                 \n",
            "                             'encoder.down.0.block.1.conv2.bias',                                   \n",
            "                             'encoder.down.0.downsample.conv.weight',                               \n",
            "                             'encoder.down.0.downsample.conv.bias',                                 \n",
            "                             'encoder.down.1.block.0.norm1.weight',                                 \n",
            "                             'encoder.down.1.block.0.norm1.bias',                                   \n",
            "                             'encoder.down.1.block.0.conv1.weight',                                 \n",
            "                             'encoder.down.1.block.0.conv1.bias',                                   \n",
            "                             'encoder.down.1.block.0.norm2.weight',                                 \n",
            "                             'encoder.down.1.block.0.norm2.bias',                                   \n",
            "                             'encoder.down.1.block.0.conv2.weight',                                 \n",
            "                             'encoder.down.1.block.0.conv2.bias',                                   \n",
            "                             'encoder.down.1.block.0.nin_shortcut.weight',                          \n",
            "                             'encoder.down.1.block.0.nin_shortcut.bias',                            \n",
            "                             'encoder.down.1.block.1.norm1.weight',                                 \n",
            "                             'encoder.down.1.block.1.norm1.bias',                                   \n",
            "                             'encoder.down.1.block.1.conv1.weight',                                 \n",
            "                             'encoder.down.1.block.1.conv1.bias',                                   \n",
            "                             'encoder.down.1.block.1.norm2.weight',                                 \n",
            "                             'encoder.down.1.block.1.norm2.bias',                                   \n",
            "                             'encoder.down.1.block.1.conv2.weight',                                 \n",
            "                             'encoder.down.1.block.1.conv2.bias',                                   \n",
            "                             'encoder.down.1.downsample.conv.weight',                               \n",
            "                             'encoder.down.1.downsample.conv.bias',                                 \n",
            "                             'encoder.down.2.block.0.norm1.weight',                                 \n",
            "                             'encoder.down.2.block.0.norm1.bias',                                   \n",
            "                             'encoder.down.2.block.0.conv1.weight',                                 \n",
            "                             'encoder.down.2.block.0.conv1.bias',                                   \n",
            "                             'encoder.down.2.block.0.norm2.weight',                                 \n",
            "                             'encoder.down.2.block.0.norm2.bias',                                   \n",
            "                             'encoder.down.2.block.0.conv2.weight',                                 \n",
            "                             'encoder.down.2.block.0.conv2.bias',                                   \n",
            "                             'encoder.down.2.block.0.nin_shortcut.weight',                          \n",
            "                             'encoder.down.2.block.0.nin_shortcut.bias',                            \n",
            "                             'encoder.down.2.block.1.norm1.weight',                                 \n",
            "                             'encoder.down.2.block.1.norm1.bias',                                   \n",
            "                             'encoder.down.2.block.1.conv1.weight',                                 \n",
            "                             'encoder.down.2.block.1.conv1.bias',                                   \n",
            "                             'encoder.down.2.block.1.norm2.weight',                                 \n",
            "                             'encoder.down.2.block.1.norm2.bias',                                   \n",
            "                             'encoder.down.2.block.1.conv2.weight',                                 \n",
            "                             'encoder.down.2.block.1.conv2.bias',                                   \n",
            "                             'encoder.down.2.downsample.conv.weight',                               \n",
            "                             'encoder.down.2.downsample.conv.bias',                                 \n",
            "                             'encoder.down.3.block.0.norm1.weight',                                 \n",
            "                             'encoder.down.3.block.0.norm1.bias',                                   \n",
            "                             'encoder.down.3.block.0.conv1.weight',                                 \n",
            "                             'encoder.down.3.block.0.conv1.bias',                                   \n",
            "                             'encoder.down.3.block.0.norm2.weight',                                 \n",
            "                             'encoder.down.3.block.0.norm2.bias',                                   \n",
            "                             'encoder.down.3.block.0.conv2.weight',                                 \n",
            "                             'encoder.down.3.block.0.conv2.bias',                                   \n",
            "                             'encoder.down.3.block.1.norm1.weight',                                 \n",
            "                             'encoder.down.3.block.1.norm1.bias',                                   \n",
            "                             'encoder.down.3.block.1.conv1.weight',                                 \n",
            "                             'encoder.down.3.block.1.conv1.bias',                                   \n",
            "                             'encoder.down.3.block.1.norm2.weight',                                 \n",
            "                             'encoder.down.3.block.1.norm2.bias',                                   \n",
            "                             'encoder.down.3.block.1.conv2.weight',                                 \n",
            "                             'encoder.down.3.block.1.conv2.bias',                                   \n",
            "                             'encoder.mid.block_1.norm1.weight',                                    \n",
            "                             'encoder.mid.block_1.norm1.bias',                                      \n",
            "                             'encoder.mid.block_1.conv1.weight',                                    \n",
            "                             'encoder.mid.block_1.conv1.bias',                                      \n",
            "                             'encoder.mid.block_1.norm2.weight',                                    \n",
            "                             'encoder.mid.block_1.norm2.bias',                                      \n",
            "                             'encoder.mid.block_1.conv2.weight',                                    \n",
            "                             'encoder.mid.block_1.conv2.bias',                                      \n",
            "                             'encoder.mid.attn_1.norm.weight',                                      \n",
            "                             'encoder.mid.attn_1.norm.bias',                                        \n",
            "                             'encoder.mid.attn_1.q.weight',                                         \n",
            "                             'encoder.mid.attn_1.q.bias',                                           \n",
            "                             'encoder.mid.attn_1.k.weight',                                         \n",
            "                             'encoder.mid.attn_1.k.bias',                                           \n",
            "                             'encoder.mid.attn_1.v.weight',                                         \n",
            "                             'encoder.mid.attn_1.v.bias',                                           \n",
            "                             'encoder.mid.attn_1.proj_out.weight',                                  \n",
            "                             'encoder.mid.attn_1.proj_out.bias',                                    \n",
            "                             'encoder.mid.block_2.norm1.weight',                                    \n",
            "                             'encoder.mid.block_2.norm1.bias',                                      \n",
            "                             'encoder.mid.block_2.conv1.weight',                                    \n",
            "                             'encoder.mid.block_2.conv1.bias',                                      \n",
            "                             'encoder.mid.block_2.norm2.weight',                                    \n",
            "                             'encoder.mid.block_2.norm2.bias',                                      \n",
            "                             'encoder.mid.block_2.conv2.weight',                                    \n",
            "                             'encoder.mid.block_2.conv2.bias',                                      \n",
            "                             'encoder.norm_out.weight', 'encoder.norm_out.bias',                    \n",
            "                             'encoder.conv_out.weight', 'encoder.conv_out.bias',                    \n",
            "                             'decoder.conv_in.weight', 'decoder.conv_in.bias',                      \n",
            "                             'decoder.mid.block_1.norm1.weight',                                    \n",
            "                             'decoder.mid.block_1.norm1.bias',                                      \n",
            "                             'decoder.mid.block_1.conv1.weight',                                    \n",
            "                             'decoder.mid.block_1.conv1.bias',                                      \n",
            "                             'decoder.mid.block_1.norm2.weight',                                    \n",
            "                             'decoder.mid.block_1.norm2.bias',                                      \n",
            "                             'decoder.mid.block_1.conv2.weight',                                    \n",
            "                             'decoder.mid.block_1.conv2.bias',                                      \n",
            "                             'decoder.mid.attn_1.norm.weight',                                      \n",
            "                             'decoder.mid.attn_1.norm.bias',                                        \n",
            "                             'decoder.mid.attn_1.q.weight',                                         \n",
            "                             'decoder.mid.attn_1.q.bias',                                           \n",
            "                             'decoder.mid.attn_1.k.weight',                                         \n",
            "                             'decoder.mid.attn_1.k.bias',                                           \n",
            "                             'decoder.mid.attn_1.v.weight',                                         \n",
            "                             'decoder.mid.attn_1.v.bias',                                           \n",
            "                             'decoder.mid.attn_1.proj_out.weight',                                  \n",
            "                             'decoder.mid.attn_1.proj_out.bias',                                    \n",
            "                             'decoder.mid.block_2.norm1.weight',                                    \n",
            "                             'decoder.mid.block_2.norm1.bias',                                      \n",
            "                             'decoder.mid.block_2.conv1.weight',                                    \n",
            "                             'decoder.mid.block_2.conv1.bias',                                      \n",
            "                             'decoder.mid.block_2.norm2.weight',                                    \n",
            "                             'decoder.mid.block_2.norm2.bias',                                      \n",
            "                             'decoder.mid.block_2.conv2.weight',                                    \n",
            "                             'decoder.mid.block_2.conv2.bias',                                      \n",
            "                             'decoder.up.0.block.0.norm1.weight',                                   \n",
            "                             'decoder.up.0.block.0.norm1.bias',                                     \n",
            "                             'decoder.up.0.block.0.conv1.weight',                                   \n",
            "                             'decoder.up.0.block.0.conv1.bias',                                     \n",
            "                             'decoder.up.0.block.0.norm2.weight',                                   \n",
            "                             'decoder.up.0.block.0.norm2.bias',                                     \n",
            "                             'decoder.up.0.block.0.conv2.weight',                                   \n",
            "                             'decoder.up.0.block.0.conv2.bias',                                     \n",
            "                             'decoder.up.0.block.0.nin_shortcut.weight',                            \n",
            "                             'decoder.up.0.block.0.nin_shortcut.bias',                              \n",
            "                             'decoder.up.0.block.1.norm1.weight',                                   \n",
            "                             'decoder.up.0.block.1.norm1.bias',                                     \n",
            "                             'decoder.up.0.block.1.conv1.weight',                                   \n",
            "                             'decoder.up.0.block.1.conv1.bias',                                     \n",
            "                             'decoder.up.0.block.1.norm2.weight',                                   \n",
            "                             'decoder.up.0.block.1.norm2.bias',                                     \n",
            "                             'decoder.up.0.block.1.conv2.weight',                                   \n",
            "                             'decoder.up.0.block.1.conv2.bias',                                     \n",
            "                             'decoder.up.0.block.2.norm1.weight',                                   \n",
            "                             'decoder.up.0.block.2.norm1.bias',                                     \n",
            "                             'decoder.up.0.block.2.conv1.weight',                                   \n",
            "                             'decoder.up.0.block.2.conv1.bias',                                     \n",
            "                             'decoder.up.0.block.2.norm2.weight',                                   \n",
            "                             'decoder.up.0.block.2.norm2.bias',                                     \n",
            "                             'decoder.up.0.block.2.conv2.weight',                                   \n",
            "                             'decoder.up.0.block.2.conv2.bias',                                     \n",
            "                             'decoder.up.1.block.0.norm1.weight',                                   \n",
            "                             'decoder.up.1.block.0.norm1.bias',                                     \n",
            "                             'decoder.up.1.block.0.conv1.weight',                                   \n",
            "                             'decoder.up.1.block.0.conv1.bias',                                     \n",
            "                             'decoder.up.1.block.0.norm2.weight',                                   \n",
            "                             'decoder.up.1.block.0.norm2.bias',                                     \n",
            "                             'decoder.up.1.block.0.conv2.weight',                                   \n",
            "                             'decoder.up.1.block.0.conv2.bias',                                     \n",
            "                             'decoder.up.1.block.0.nin_shortcut.weight',                            \n",
            "                             'decoder.up.1.block.0.nin_shortcut.bias',                              \n",
            "                             'decoder.up.1.block.1.norm1.weight',                                   \n",
            "                             'decoder.up.1.block.1.norm1.bias',                                     \n",
            "                             'decoder.up.1.block.1.conv1.weight',                                   \n",
            "                             'decoder.up.1.block.1.conv1.bias',                                     \n",
            "                             'decoder.up.1.block.1.norm2.weight',                                   \n",
            "                             'decoder.up.1.block.1.norm2.bias',                                     \n",
            "                             'decoder.up.1.block.1.conv2.weight',                                   \n",
            "                             'decoder.up.1.block.1.conv2.bias',                                     \n",
            "                             'decoder.up.1.block.2.norm1.weight',                                   \n",
            "                             'decoder.up.1.block.2.norm1.bias',                                     \n",
            "                             'decoder.up.1.block.2.conv1.weight',                                   \n",
            "                             'decoder.up.1.block.2.conv1.bias',                                     \n",
            "                             'decoder.up.1.block.2.norm2.weight',                                   \n",
            "                             'decoder.up.1.block.2.norm2.bias',                                     \n",
            "                             'decoder.up.1.block.2.conv2.weight',                                   \n",
            "                             'decoder.up.1.block.2.conv2.bias',                                     \n",
            "                             'decoder.up.1.upsample.conv.weight',                                   \n",
            "                             'decoder.up.1.upsample.conv.bias',                                     \n",
            "                             'decoder.up.2.block.0.norm1.weight',                                   \n",
            "                             'decoder.up.2.block.0.norm1.bias',                                     \n",
            "                             'decoder.up.2.block.0.conv1.weight',                                   \n",
            "                             'decoder.up.2.block.0.conv1.bias',                                     \n",
            "                             'decoder.up.2.block.0.norm2.weight',                                   \n",
            "                             'decoder.up.2.block.0.norm2.bias',                                     \n",
            "                             'decoder.up.2.block.0.conv2.weight',                                   \n",
            "                             'decoder.up.2.block.0.conv2.bias',                                     \n",
            "                             'decoder.up.2.block.1.norm1.weight',                                   \n",
            "                             'decoder.up.2.block.1.norm1.bias',                                     \n",
            "                             'decoder.up.2.block.1.conv1.weight',                                   \n",
            "                             'decoder.up.2.block.1.conv1.bias',                                     \n",
            "                             'decoder.up.2.block.1.norm2.weight',                                   \n",
            "                             'decoder.up.2.block.1.norm2.bias',                                     \n",
            "                             'decoder.up.2.block.1.conv2.weight',                                   \n",
            "                             'decoder.up.2.block.1.conv2.bias',                                     \n",
            "                             'decoder.up.2.block.2.norm1.weight',                                   \n",
            "                             'decoder.up.2.block.2.norm1.bias',                                     \n",
            "                             'decoder.up.2.block.2.conv1.weight',                                   \n",
            "                             'decoder.up.2.block.2.conv1.bias',                                     \n",
            "                             'decoder.up.2.block.2.norm2.weight',                                   \n",
            "                             'decoder.up.2.block.2.norm2.bias',                                     \n",
            "                             'decoder.up.2.block.2.conv2.weight',                                   \n",
            "                             'decoder.up.2.block.2.conv2.bias',                                     \n",
            "                             'decoder.up.2.upsample.conv.weight',                                   \n",
            "                             'decoder.up.2.upsample.conv.bias',                                     \n",
            "                             'decoder.up.3.block.0.norm1.weight',                                   \n",
            "                             'decoder.up.3.block.0.norm1.bias',                                     \n",
            "                             'decoder.up.3.block.0.conv1.weight',                                   \n",
            "                             'decoder.up.3.block.0.conv1.bias',                                     \n",
            "                             'decoder.up.3.block.0.norm2.weight',                                   \n",
            "                             'decoder.up.3.block.0.norm2.bias',                                     \n",
            "                             'decoder.up.3.block.0.conv2.weight',                                   \n",
            "                             'decoder.up.3.block.0.conv2.bias',                                     \n",
            "                             'decoder.up.3.block.1.norm1.weight',                                   \n",
            "                             'decoder.up.3.block.1.norm1.bias',                                     \n",
            "                             'decoder.up.3.block.1.conv1.weight',                                   \n",
            "                             'decoder.up.3.block.1.conv1.bias',                                     \n",
            "                             'decoder.up.3.block.1.norm2.weight',                                   \n",
            "                             'decoder.up.3.block.1.norm2.bias',                                     \n",
            "                             'decoder.up.3.block.1.conv2.weight',                                   \n",
            "                             'decoder.up.3.block.1.conv2.bias',                                     \n",
            "                             'decoder.up.3.block.2.norm1.weight',                                   \n",
            "                             'decoder.up.3.block.2.norm1.bias',                                     \n",
            "                             'decoder.up.3.block.2.conv1.weight',                                   \n",
            "                             'decoder.up.3.block.2.conv1.bias',                                     \n",
            "                             'decoder.up.3.block.2.norm2.weight',                                   \n",
            "                             'decoder.up.3.block.2.norm2.bias',                                     \n",
            "                             'decoder.up.3.block.2.conv2.weight',                                   \n",
            "                             'decoder.up.3.block.2.conv2.bias',                                     \n",
            "                             'decoder.up.3.upsample.conv.weight',                                   \n",
            "                             'decoder.up.3.upsample.conv.bias',                                     \n",
            "                             'decoder.norm_out.weight', 'decoder.norm_out.bias',                    \n",
            "                             'decoder.conv_out.weight', 'decoder.conv_out.bias'],                   \n",
            "                             unexpected_keys=['lora_te_text_model_encoder_layers_0                  \n",
            "                             _mlp_fc1.alpha',                                                       \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_0_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc1.alpha',                  \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc1.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc2.alpha',                  \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc2.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_k_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_k_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_k_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_out_p                  \n",
            "                             roj.alpha',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_out_p                  \n",
            "                             roj.lora_down.weight',                                                 \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_out_p                  \n",
            "                             roj.lora_up.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_q_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_q_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_q_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_v_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_v_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_10_self_attn_v_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc1.alpha',                  \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc1.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc2.alpha',                  \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc2.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_k_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_k_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_k_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_out_p                  \n",
            "                             roj.alpha',                                                            \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_out_p                  \n",
            "                             roj.lora_down.weight',                                                 \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_out_p                  \n",
            "                             roj.lora_up.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_q_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_q_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_q_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_v_pro                  \n",
            "                             j.alpha',                                                              \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_v_pro                  \n",
            "                             j.lora_down.weight',                                                   \n",
            "                             'lora_te_text_model_encoder_layers_11_self_attn_v_pro                  \n",
            "                             j.lora_up.weight',                                                     \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_1_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_2_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_3_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_4_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_5_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_6_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_7_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_8_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc1.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc1.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc2.alpha',                   \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc2.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_k_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_k_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_k_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_out_pr                  \n",
            "                             oj.alpha',                                                             \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_out_pr                  \n",
            "                             oj.lora_down.weight',                                                  \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_out_pr                  \n",
            "                             oj.lora_up.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_q_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_q_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_q_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_v_proj                  \n",
            "                             .alpha',                                                               \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_v_proj                  \n",
            "                             .lora_down.weight',                                                    \n",
            "                             'lora_te_text_model_encoder_layers_9_self_attn_v_proj                  \n",
            "                             .lora_up.weight',                                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_0_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_1_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_0_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_0_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_1_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_1_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_0_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_0_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_in.alpha',                  \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_in.lora_do                  \n",
            "                             wn.weight',                                                            \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_in.lora_up                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_out.alpha'                  \n",
            "                             ,                                                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_out.lora_d                  \n",
            "                             own.weight',                                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_1_proj_out.lora_u                  \n",
            "                             p.weight',                                                             \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn1_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_k.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.alpha',                                           \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_down.weight',                                \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_out_0.lora_up.weight',                                  \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_q.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.alpha',                                               \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_down.weight',                                    \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_attn2_to_v.lora_up.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.alpha',                                            \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_down.weight',                                 \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_0_proj.lora_up.weight',                                   \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.alpha',                                                 \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_down.weight',                                      \n",
            "                             'lora_unet_down_blocks_2_attentions_1_transformer_blo                  \n",
            "                             cks_0_ff_net_2.lora_up.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_proj_in.alpha',                      \n",
            "                             'lora_unet_mid_block_attentions_0_proj_in.lora_down.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_mid_block_attentions_0_proj_in.lora_up.wei                  \n",
            "                             ght',                                                                  \n",
            "                             'lora_unet_mid_block_attentions_0_proj_out.alpha',                     \n",
            "                             'lora_unet_mid_block_attentions_0_proj_out.lora_down.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_mid_block_attentions_0_proj_out.lora_up.we                  \n",
            "                             ight',                                                                 \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_k.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_k.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_k.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_out_0.alpha',                                               \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_out_0.lora_down.weight',                                    \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_out_0.lora_up.weight',                                      \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_q.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_q.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_q.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_v.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_v.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn1_to_v.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_k.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_k.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_k.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_out_0.alpha',                                               \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_out_0.lora_down.weight',                                    \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_out_0.lora_up.weight',                                      \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_q.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_q.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_q.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_v.alpha',                                                   \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_v.lora_down.weight',                                        \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_attn2_to_v.lora_up.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_0_proj.alpha',                                                \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_0_proj.lora_down.weight',                                     \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_0_proj.lora_up.weight',                                       \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_2.alpha',                                                     \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_2.lora_down.weight',                                          \n",
            "                             'lora_unet_mid_block_attentions_0_transformer_blocks_                  \n",
            "                             0_ff_net_2.lora_up.weight',                                            \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_0_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_1_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_2_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_0_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_1_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight',                                          \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.alpha',                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.lora_down                  \n",
            "                             .weight',                                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.w                  \n",
            "                             eight',                                                                \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.alpha',                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.lora_dow                  \n",
            "                             n.weight',                                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.                  \n",
            "                             weight',                                                               \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn1_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_k.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.alpha',                                             \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_down.weight',                                  \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_out_0.lora_up.weight',                                    \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_q.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.alpha',                                                 \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_down.weight',                                      \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_attn2_to_v.lora_up.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.alpha',                                              \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_down.weight',                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_0_proj.lora_up.weight',                                     \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.alpha',                                                   \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_down.weight',                                        \n",
            "                             'lora_unet_up_blocks_3_attentions_2_transformer_block                  \n",
            "                             s_0_ff_net_2.lora_up.weight'])                                         \n",
            "import network module: .networks.lora_flux\n",
            "                    ERROR    !!! Exception during processing !!! Cannot copy out of execution.py:396\n",
            "                             meta tensor; no data! Please use                                       \n",
            "                             torch.nn.Module.to_empty() instead of                                  \n",
            "                             torch.nn.Module.to() when moving module from meta to a                 \n",
            "                             different device.                                                      \n",
            "                    ERROR    Traceback (most recent call last):                     execution.py:397\n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 327, in execute                                                   \n",
            "                                 output_data, output_ui, has_subgraph =                             \n",
            "                             get_output_data(obj, input_data_all,                                   \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                                        ^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                              \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 202, in get_output_data                                           \n",
            "                                 return_values = _map_node_over_list(obj,                           \n",
            "                             input_data_all, obj.FUNCTION, allow_interrupt=True,                    \n",
            "                             execution_block_cb=execution_block_cb,                                 \n",
            "                             pre_execute_cb=pre_execute_cb)                                         \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                 \n",
            "                             ^^^^                                                                   \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 174, in _map_node_over_list                                       \n",
            "                                 process_inputs(input_dict, i)                                      \n",
            "                               File \"/content/drive/MyDrive/ComfyUI/execution.py\",                  \n",
            "                             line 163, in process_inputs                                            \n",
            "                                 results.append(getattr(obj, func)(**inputs))                       \n",
            "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                        \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/nodes.py\", line 631, in init_training                       \n",
            "                                 training_loop = network_trainer.init_train(args)                   \n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                   \n",
            "                               File                                                                 \n",
            "                             \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-f                 \n",
            "                             luxtrainer/train_network.py\", line 449, in init_train                  \n",
            "                                 vae.to(accelerator.device, dtype=vae_dtype)                        \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 1343, in to                                       \n",
            "                                 return self._apply(convert)                                        \n",
            "                                        ^^^^^^^^^^^^^^^^^^^^                                        \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 903, in _apply                                    \n",
            "                                 module._apply(fn)                                                  \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 903, in _apply                                    \n",
            "                                 module._apply(fn)                                                  \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 930, in _apply                                    \n",
            "                                 param_applied = fn(param)                                          \n",
            "                                                 ^^^^^^^^^                                          \n",
            "                               File                                                                 \n",
            "                             \"/usr/local/lib/python3.11/dist-packages/torch/nn/modu                 \n",
            "                             les/module.py\", line 1336, in convert                                  \n",
            "                                 raise NotImplementedError(                                         \n",
            "                             NotImplementedError: Cannot copy out of meta tensor;                   \n",
            "                             no data! Please use torch.nn.Module.to_empty() instead                 \n",
            "                             of torch.nn.Module.to() when moving module from meta                   \n",
            "                             to a different device.                                                 \n",
            "                                                                                                    \n",
            "                    INFO     Prompt executed in 3.29 seconds                             main.py:189\n",
            "FETCH DATA from: /content/drive/MyDrive/ComfyUI/user/default/ComfyUI-Manager/cache/4245046894_model-list.json [DONE]\n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
        "\n",
        "  print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
        "  for line in p.stdout:\n",
        "    print(line.decode(), end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gggggggggg"
      },
      "source": [
        "### Run ComfyUI with colab iframe (use only in case the previous way with localtunnel doesn't work)\n",
        "\n",
        "You should see the ui appear in an iframe. If you get a 403 error, it's your firefox settings or an extension that's messing things up.\n",
        "\n",
        "If you want to open it in another window use the link.\n",
        "\n",
        "Note that some UI features like live image previews won't work because the colab iframe blocks websockets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhhhhhhhh"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "import socket\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  from google.colab import output\n",
        "  output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "  print(\"to open it in a window you can open this link here:\")\n",
        "  output.serve_kernel_port_as_window(port)\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}